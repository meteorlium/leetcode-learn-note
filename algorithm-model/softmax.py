import numpy as np
import matplotlib.pyplot as plt


class Softmax:
    def __init__(self):
        """
        Softmaxæ¿€æ´»å‡½æ•°å®ç° - æ·±åº¦å­¦ä¹ ä¸­çš„é‡è¦æ¿€æ´»å‡½æ•°
        
        Softmaxçš„æ ¸å¿ƒä½œç”¨ï¼š
        1. å°†ä»»æ„å®æ•°å‘é‡è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒ
        2. è¾“å‡ºå€¼åœ¨(0,1)ä¹‹é—´ä¸”å’Œä¸º1
        3. å¸¸ç”¨äºå¤šåˆ†ç±»é—®é¢˜çš„è¾“å‡ºå±‚
        
        æ•°å­¦å…¬å¼ï¼šsoftmax(xi) = exp(xi) / Î£(exp(xj))
        """
        pass
    
    def forward(self, x, axis=-1):
        """
        Softmaxå‰å‘ä¼ æ’­ - æ ¸å¿ƒè®¡ç®—å‡½æ•°
        
        Args:
            x: è¾“å…¥å¼ é‡ (ä»»æ„å½¢çŠ¶)
            axis: è®¡ç®—softmaxçš„ç»´åº¦ï¼Œé»˜è®¤ä¸ºæœ€åä¸€ç»´
            
        Returns:
            è¾“å‡ºæ¦‚ç‡åˆ†å¸ƒ
            
        ã€é¢è¯•è€ƒç‚¹1ã€‘ï¼šæ•°å€¼ç¨³å®šæ€§å¤„ç†
        ç›´æ¥è®¡ç®—exp(x)å®¹æ˜“æ•°å€¼æº¢å‡ºï¼Œå‡å»æœ€å¤§å€¼ä¿è¯æ•°å€¼ç¨³å®š
        softmax(x) = softmax(x - max(x)) æ•°å­¦ä¸Šç­‰ä»·
        """
        # ã€é¢è¯•è€ƒç‚¹2ã€‘ï¼šé˜²æ­¢æ•°å€¼æº¢å‡ºçš„å…³é”®æ­¥éª¤
        # å‡å»æœ€å¤§å€¼ä¸æ”¹å˜softmaxç»“æœä½†é¿å…exp()æº¢å‡º
        x_stable = x - np.max(x, axis=axis, keepdims=True)
        
        # ã€é¢è¯•è€ƒç‚¹3ã€‘ï¼šæŒ‡æ•°è¿ç®—
        # expå‡½æ•°å°†è¾“å…¥æ˜ å°„åˆ°æ­£æ•°åŸŸï¼Œä¿è¯æ¦‚ç‡éè´Ÿ
        exp_x = np.exp(x_stable)
        
        # ã€é¢è¯•è€ƒç‚¹4ã€‘ï¼šå½’ä¸€åŒ–æ­¥éª¤
        # é™¤ä»¥æ€»å’Œä½¿å¾—è¾“å‡ºä¸ºæ¦‚ç‡åˆ†å¸ƒï¼ˆå’Œä¸º1ï¼‰
        softmax_output = exp_x / np.sum(exp_x, axis=axis, keepdims=True)
        
        return softmax_output
    
    def backward(self, y, grad_output):
        """
        Softmaxåå‘ä¼ æ’­ - æ¢¯åº¦è®¡ç®—
        
        Args:
            y: softmaxçš„è¾“å‡º (å‰å‘ä¼ æ’­ç»“æœ)
            grad_output: æ¥è‡ªä¸Šå±‚çš„æ¢¯åº¦
            
        Returns:
            å¯¹è¾“å…¥xçš„æ¢¯åº¦
            
        ã€é¢è¯•è€ƒç‚¹5ã€‘ï¼šSoftmaxæ¢¯åº¦æ¨å¯¼
        âˆ‚softmax(xi)/âˆ‚xj = softmax(xi) * (Î´ij - softmax(xj))
        å…¶ä¸­Î´ijæ˜¯Kronecker deltaå‡½æ•°
        """
        # ã€é¢è¯•è€ƒç‚¹6ã€‘ï¼šé›…å¯æ¯”çŸ©é˜µè®¡ç®—
        # Softmaxçš„æ¢¯åº¦æ˜¯ä¸€ä¸ªé›…å¯æ¯”çŸ©é˜µï¼Œä¸æ˜¯ç®€å•çš„å…ƒç´ å¯¹å…ƒç´ 
        batch_size = y.shape[0]
        num_classes = y.shape[1] if len(y.shape) > 1 else len(y)
        
        if len(y.shape) == 1:
            # ä¸€ç»´æƒ…å†µ
            grad_input = np.zeros_like(y)
            for i in range(len(y)):
                for j in range(len(y)):
                    if i == j:
                        grad_input[i] += grad_output[j] * y[i] * (1 - y[j])
                    else:
                        grad_input[i] += grad_output[j] * y[i] * (-y[j])
        else:
            # æ‰¹å¤„ç†æƒ…å†µ
            grad_input = np.zeros_like(y)
            for b in range(batch_size):
                for i in range(num_classes):
                    for j in range(num_classes):
                        if i == j:
                            grad_input[b, i] += grad_output[b, j] * y[b, i] * (1 - y[b, j])
                        else:
                            grad_input[b, i] += grad_output[b, j] * y[b, i] * (-y[b, j])
        
        return grad_input
    
    def cross_entropy_loss(self, predictions, targets):
        """
        äº¤å‰ç†µæŸå¤±å‡½æ•° - Softmaxå¸¸ç”¨çš„æŸå¤±å‡½æ•°
        
        Args:
            predictions: softmaxè¾“å‡º (batch_size, num_classes)
            targets: çœŸå®æ ‡ç­¾ (batch_size, num_classes) one-hotç¼–ç 
            
        Returns:
            äº¤å‰ç†µæŸå¤±å€¼
            
        ã€é¢è¯•è€ƒç‚¹7ã€‘ï¼šä¸ºä»€ä¹ˆSoftmaxå’Œäº¤å‰ç†µé…åˆä½¿ç”¨ï¼Ÿ
        1. æ•°å­¦ä¸Šä¼˜é›…ï¼šç»„åˆåæ¢¯åº¦å½¢å¼ç®€å•
        2. æ•°å€¼ç¨³å®šï¼šå¯ä»¥ç›´æ¥ä»logitsè®¡ç®—é¿å…ä¸­é—´æ­¥éª¤
        3. æ¦‚ç‡è§£é‡Šï¼šæœ€å¤§ä¼¼ç„¶ä¼°è®¡çš„è‡ªç„¶ç»“æœ
        """
        # ã€é¢è¯•è€ƒç‚¹8ã€‘ï¼šæ•°å€¼ç¨³å®šçš„æŸå¤±è®¡ç®—
        # æ·»åŠ å°å¸¸æ•°é¿å…log(0)
        epsilon = 1e-15
        predictions = np.clip(predictions, epsilon, 1 - epsilon)
        
        # ã€é¢è¯•è€ƒç‚¹9ã€‘ï¼šäº¤å‰ç†µå…¬å¼ H(p,q) = -Î£ p(x)log(q(x))
        loss = -np.sum(targets * np.log(predictions), axis=1)
        return np.mean(loss)
    
    def softmax_with_logits(self, logits, targets):
        """
        æ•°å€¼ç¨³å®šçš„Softmax + äº¤å‰ç†µç»„åˆè®¡ç®—
        
        ã€é¢è¯•è€ƒç‚¹10ã€‘ï¼šå·¥ç¨‹å®ç°æŠ€å·§
        ç›´æ¥ä»logitsè®¡ç®—æŸå¤±ï¼Œé¿å…ä¸­é—´çš„softmaxæ­¥éª¤ï¼Œæ›´ç¨³å®š
        """
        # æ•°å€¼ç¨³å®šçš„å®ç°
        logits_stable = logits - np.max(logits, axis=-1, keepdims=True)
        log_sum_exp = np.log(np.sum(np.exp(logits_stable), axis=-1, keepdims=True))
        log_softmax = logits_stable - log_sum_exp
        
        # äº¤å‰ç†µæŸå¤±
        loss = -np.sum(targets * log_softmax, axis=1)
        return np.mean(loss), np.exp(log_softmax)


def demonstrate_softmax_properties():
    """
    æ¼”ç¤ºSoftmaxçš„å…³é”®æ€§è´¨ - é¢è¯•å¸¸é—®é—®é¢˜
    """
    print("=== Softmaxç®—æ³•æ€§è´¨æ¼”ç¤º ===")
    
    softmax = Softmax()
    
    # ã€é¢è¯•è€ƒç‚¹11ã€‘ï¼šä¸åŒè¾“å…¥çš„Softmaxè¡Œä¸º
    print("\n1. åŸºæœ¬è®¡ç®—ç¤ºä¾‹:")
    x1 = np.array([1.0, 2.0, 3.0])
    y1 = softmax.forward(x1)
    print(f"è¾“å…¥: {x1}")
    print(f"è¾“å‡º: {y1}")
    print(f"å’Œä¸º1éªŒè¯: {np.sum(y1):.6f}")
    
    # ã€é¢è¯•è€ƒç‚¹12ã€‘ï¼šæ¸©åº¦å‚æ•°çš„å½±å“
    print("\n2. æ¸©åº¦å‚æ•°æ•ˆåº”:")
    x = np.array([1.0, 2.0, 3.0])
    temperatures = [0.5, 1.0, 2.0, 5.0]
    
    for temp in temperatures:
        y_temp = softmax.forward(x / temp)
        print(f"æ¸©åº¦={temp}: {y_temp} (æœ€å¤§å€¼å æ¯”: {np.max(y_temp):.3f})")
    
    print("\nã€é¢è¯•é‡ç‚¹ã€‘æ¸©åº¦å‚æ•°ä½œç”¨:")
    print("- æ¸©åº¦ < 1: è¾“å‡ºæ›´åŠ é”åŒ–ï¼Œæ¥è¿‘one-hot")
    print("- æ¸©åº¦ > 1: è¾“å‡ºæ›´åŠ å¹³æ»‘ï¼Œæ¥è¿‘å‡åŒ€åˆ†å¸ƒ")
    print("- æ¸©åº¦ â†’ 0: è¾“å‡ºæ¥è¿‘ç¡¬æ€§æœ€å¤§å€¼")
    print("- æ¸©åº¦ â†’ âˆ: è¾“å‡ºæ¥è¿‘å‡åŒ€åˆ†å¸ƒ")
    
    # ã€é¢è¯•è€ƒç‚¹13ã€‘ï¼šå¤§æ•°å€¼è¾“å…¥çš„ç¨³å®šæ€§
    print("\n3. æ•°å€¼ç¨³å®šæ€§éªŒè¯:")
    x_large = np.array([1000.0, 1001.0, 1002.0])
    try:
        y_naive = np.exp(x_large) / np.sum(np.exp(x_large))
        print(f"ç›´æ¥è®¡ç®—: {y_naive}")
    except:
        print("ç›´æ¥è®¡ç®—æº¢å‡º!")
    
    y_stable = softmax.forward(x_large)
    print(f"ç¨³å®šè®¡ç®—: {y_stable}")
    
    return softmax


def test_gradient_computation():
    """
    æµ‹è¯•æ¢¯åº¦è®¡ç®— - åå‘ä¼ æ’­éªŒè¯
    
    ã€é¢è¯•è€ƒç‚¹14ã€‘ï¼šæ•°å€¼æ¢¯åº¦æ£€éªŒ
    ä½¿ç”¨æœ‰é™å·®åˆ†æ³•éªŒè¯è§£ææ¢¯åº¦çš„æ­£ç¡®æ€§
    """
    print("\n=== æ¢¯åº¦è®¡ç®—éªŒè¯ ===")
    
    softmax = Softmax()
    
    # ç®€å•æµ‹è¯•ç”¨ä¾‹
    x = np.array([1.0, 2.0, 3.0])
    y = softmax.forward(x)
    
    # æ¨¡æ‹Ÿæ¥è‡ªä¸Šå±‚çš„æ¢¯åº¦
    grad_output = np.array([0.1, -0.2, 0.1])
    
    # è§£ææ¢¯åº¦
    grad_analytical = softmax.backward(y, grad_output)
    
    # æ•°å€¼æ¢¯åº¦ï¼ˆæœ‰é™å·®åˆ†ï¼‰
    epsilon = 1e-7
    grad_numerical = np.zeros_like(x)
    
    for i in range(len(x)):
        x_plus = x.copy()
        x_minus = x.copy()
        x_plus[i] += epsilon
        x_minus[i] -= epsilon
        
        y_plus = softmax.forward(x_plus)
        y_minus = softmax.forward(x_minus)
        
        # ä½¿ç”¨é“¾å¼æ³•åˆ™è®¡ç®—æ•°å€¼æ¢¯åº¦
        grad_numerical[i] = np.sum(grad_output * (y_plus - y_minus)) / (2 * epsilon)
    
    print(f"è§£ææ¢¯åº¦: {grad_analytical}")
    print(f"æ•°å€¼æ¢¯åº¦: {grad_numerical}")
    print(f"å·®å¼‚: {np.abs(grad_analytical - grad_numerical)}")
    
    # ã€é¢è¯•è€ƒç‚¹15ã€‘ï¼šæ¢¯åº¦æ£€éªŒæ ‡å‡†
    tolerance = 1e-5
    is_correct = np.allclose(grad_analytical, grad_numerical, atol=tolerance)
    print(f"æ¢¯åº¦éªŒè¯{'é€šè¿‡' if is_correct else 'å¤±è´¥'} (å®¹å¿åº¦: {tolerance})")


def classification_example():
    """
    å¤šåˆ†ç±»ä»»åŠ¡ç¤ºä¾‹ - å®é™…åº”ç”¨åœºæ™¯
    
    ã€é¢è¯•è€ƒç‚¹16ã€‘ï¼šSoftmaxåœ¨å®é™…é—®é¢˜ä¸­çš„åº”ç”¨
    """
    print("\n=== å¤šåˆ†ç±»ä»»åŠ¡ç¤ºä¾‹ ===")
    
    softmax = Softmax()
    
    # æ¨¡æ‹Ÿ3åˆ†ç±»é—®é¢˜çš„logits
    # å‡è®¾æ˜¯å›¾åƒåˆ†ç±»ï¼šçŒ«ã€ç‹—ã€é¸Ÿ
    logits = np.array([
        [2.3, 1.1, 0.5],    # æ ·æœ¬1: æ›´å¯èƒ½æ˜¯çŒ«
        [0.8, 2.7, 1.2],    # æ ·æœ¬2: æ›´å¯èƒ½æ˜¯ç‹—  
        [1.0, 0.9, 2.8],    # æ ·æœ¬3: æ›´å¯èƒ½æ˜¯é¸Ÿ
    ])
    
    # çœŸå®æ ‡ç­¾ (one-hotç¼–ç )
    targets = np.array([
        [1, 0, 0],  # æ ·æœ¬1ç¡®å®æ˜¯çŒ«
        [0, 1, 0],  # æ ·æœ¬2ç¡®å®æ˜¯ç‹—
        [0, 0, 1],  # æ ·æœ¬3ç¡®å®æ˜¯é¸Ÿ
    ])
    
    # è®¡ç®—é¢„æµ‹æ¦‚ç‡
    predictions = softmax.forward(logits, axis=1)
    
    print("åˆ†ç±»ç»“æœ:")
    class_names = ['çŒ«', 'ç‹—', 'é¸Ÿ']
    for i, (pred, target) in enumerate(zip(predictions, targets)):
        predicted_class = np.argmax(pred)
        true_class = np.argmax(target)
        confidence = pred[predicted_class]
        
        print(f"æ ·æœ¬{i+1}: é¢„æµ‹={class_names[predicted_class]}({confidence:.3f}), "
              f"çœŸå®={class_names[true_class]}, "
              f"{'âœ“' if predicted_class == true_class else 'âœ—'}")
    
    # è®¡ç®—æŸå¤±
    loss = softmax.cross_entropy_loss(predictions, targets)
    print(f"\näº¤å‰ç†µæŸå¤±: {loss:.4f}")
    
    # ã€é¢è¯•è€ƒç‚¹17ã€‘ï¼šå‡†ç¡®ç‡è®¡ç®—
    predicted_classes = np.argmax(predictions, axis=1)
    true_classes = np.argmax(targets, axis=1)
    accuracy = np.mean(predicted_classes == true_classes)
    print(f"å‡†ç¡®ç‡: {accuracy:.3f}")


def advanced_concepts():
    """
    é«˜çº§æ¦‚å¿µå’Œé¢è¯•éš¾ç‚¹
    
    ã€é¢è¯•è€ƒç‚¹18-20ã€‘ï¼šæ·±å…¥ç†è§£
    """
    print("\n=== é«˜çº§æ¦‚å¿µ ===")
    
    print("1. Gumbel-SoftmaxæŠ€å·§:")
    print("   - ç”¨äºå¯å¾®åˆ†çš„ç¦»æ•£é‡‡æ ·")
    print("   - åœ¨å¼ºåŒ–å­¦ä¹ å’Œå˜åˆ†è‡ªç¼–ç å™¨ä¸­åº”ç”¨")
    
    print("\n2. Softmaxçš„æ›¿ä»£æ–¹æ¡ˆ:")
    print("   - Sparsemax: äº§ç”Ÿç¨€ç–æ¦‚ç‡åˆ†å¸ƒ")
    print("   - Entmax: å¯è°ƒèŠ‚ç¨€ç–ç¨‹åº¦")
    print("   - Hierarchical Softmax: é™ä½å¤§è¯æ±‡è¡¨çš„è®¡ç®—å¤æ‚åº¦")
    
    print("\n3. è®¡ç®—ä¼˜åŒ–:")
    print("   - Log-Sum-ExpæŠ€å·§: æ•°å€¼ç¨³å®šçš„å¯¹æ•°ç©ºé—´è®¡ç®—")
    print("   - å‘é‡åŒ–å®ç°: åˆ©ç”¨SIMDæŒ‡ä»¤åŠ é€Ÿ")
    print("   - å¹¶è¡ŒåŒ–: GPUä¸Šçš„é«˜æ•ˆå®ç°")
    
    # ã€é¢è¯•è€ƒç‚¹19ã€‘ï¼šSoftmax vs Sigmoid
    print("\n4. Softmax vs Sigmoidå¯¹æ¯”:")
    x = np.array([1.0, 2.0])
    softmax = Softmax()
    
    # Softmax (å¤šåˆ†ç±»)
    softmax_out = softmax.forward(x)
    
    # Sigmoid (äºŒåˆ†ç±»æˆ–å¤šæ ‡ç­¾)
    sigmoid_out = 1 / (1 + np.exp(-x))
    
    print(f"è¾“å…¥: {x}")
    print(f"Softmaxè¾“å‡º: {softmax_out} (å’Œ={np.sum(softmax_out):.3f})")
    print(f"Sigmoidè¾“å‡º: {sigmoid_out} (ç‹¬ç«‹æ¦‚ç‡)")
    
    print("\nã€å…³é”®åŒºåˆ«ã€‘:")
    print("- Softmax: æ¦‚ç‡å’Œä¸º1ï¼Œç”¨äºå•æ ‡ç­¾å¤šåˆ†ç±»")
    print("- Sigmoid: æ¯ä¸ªè¾“å‡ºç‹¬ç«‹ï¼Œç”¨äºå¤šæ ‡ç­¾åˆ†ç±»")


def test_softmax():
    """
    å®Œæ•´çš„æµ‹è¯•å‡½æ•° - ç®—æ³•å·¥ç¨‹å¸ˆé¢è¯•æ ‡å‡†
    
    ã€é¢è¯•è€ƒç‚¹21ã€‘ï¼šå…¨é¢çš„æµ‹è¯•è¦†ç›–
    """
    print("=== Softmaxç®—æ³•æ·±åº¦å­¦ä¹ é¢è¯•æµ‹è¯• ===\n")
    
    # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    softmax_demo = demonstrate_softmax_properties()
    
    # æ¢¯åº¦éªŒè¯
    test_gradient_computation()
    
    # å®é™…åº”ç”¨
    classification_example()
    
    # é«˜çº§æ¦‚å¿µ
    advanced_concepts()
    
    print("\n=== é¢è¯•æ€»ç»“ ===")
    print("âœ… æ ¸å¿ƒè€ƒç‚¹è¦†ç›–:")
    print("1. æ•°å­¦åŸç†: æŒ‡æ•°å½’ä¸€åŒ–ï¼Œæ¦‚ç‡åˆ†å¸ƒ")
    print("2. æ•°å€¼ç¨³å®šæ€§: å‡å»æœ€å¤§å€¼æŠ€å·§")
    print("3. æ¢¯åº¦è®¡ç®—: é›…å¯æ¯”çŸ©é˜µï¼Œé“¾å¼æ³•åˆ™")
    print("4. æŸå¤±å‡½æ•°: äº¤å‰ç†µï¼Œæ•°å€¼ç¨³å®šå®ç°")
    print("5. å®é™…åº”ç”¨: å¤šåˆ†ç±»ï¼Œæ¸©åº¦å‚æ•°")
    print("6. å·¥ç¨‹ä¼˜åŒ–: å‘é‡åŒ–ï¼Œå†…å­˜æ•ˆç‡")
    print("7. ç†è®ºå¯¹æ¯”: vs Sigmoid, vså…¶ä»–æ¿€æ´»å‡½æ•°")
    
    print("\nğŸ’¡ é¢è¯•é‡ç‚¹å›ç­”è¦ç‚¹:")
    print("- èƒ½è§£é‡ŠSoftmaxçš„æ•°å­¦ç›´è§‰")
    print("- çŸ¥é“æ•°å€¼ç¨³å®šæ€§çš„é‡è¦æ€§å’Œå®ç°æ–¹æ³•")
    print("- ç†è§£ä¸äº¤å‰ç†µæŸå¤±çš„é…åˆä½¿ç”¨")
    print("- æŒæ¡æ¢¯åº¦æ¨å¯¼å’Œåå‘ä¼ æ’­")
    print("- äº†è§£åœ¨ä¸åŒåœºæ™¯ä¸‹çš„åº”ç”¨å’Œé™åˆ¶")


if __name__ == "__main__":
    test_softmax()