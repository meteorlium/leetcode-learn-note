import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        """
        Multi-Head Attention implementation
        å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å®ç° - Transformeræ ¸å¿ƒç»„ä»¶
        
        Args:
            d_model: dimension of model (æ¨¡å‹ç»´åº¦)
            num_heads: number of attention heads (æ³¨æ„åŠ›å¤´æ•°)
        """
        super(MultiHeadAttention, self).__init__()
        
        # ã€é¢è¯•è€ƒç‚¹1ã€‘ï¼šç»´åº¦æ ¡éªŒ - d_modelå¿…é¡»èƒ½è¢«num_headsæ•´é™¤
        # è¿™æ˜¯å› ä¸ºæ¯ä¸ªå¤´è¦åˆ†é…ç›¸ç­‰çš„ç»´åº¦ d_k = d_model // num_heads
        assert d_model % num_heads == 0, "d_modelå¿…é¡»èƒ½è¢«num_headsæ•´é™¤"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦
        
        # ã€é¢è¯•è€ƒç‚¹2ã€‘ï¼šå‚æ•°åˆå§‹åŒ–ç­–ç•¥
        # ä½¿ç”¨PyTorchçš„nn.Linearå±‚ï¼Œè‡ªåŠ¨è¿›è¡Œæƒé‡åˆå§‹åŒ–
        # PyTorché»˜è®¤ä½¿ç”¨Kaiming uniformåˆå§‹åŒ–
        self.W_q = nn.Linear(d_model, d_model, bias=False)  # QueryæŠ•å½±çŸ©é˜µ
        self.W_k = nn.Linear(d_model, d_model, bias=False)  # KeyæŠ•å½±çŸ©é˜µ  
        self.W_v = nn.Linear(d_model, d_model, bias=False)  # ValueæŠ•å½±çŸ©é˜µ
        self.W_o = nn.Linear(d_model, d_model, bias=False)  # è¾“å‡ºæŠ•å½±çŸ©é˜µ
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """
        Scaled Dot-Product Attention - æ³¨æ„åŠ›æœºåˆ¶çš„æ ¸å¿ƒè®¡ç®—
        ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼šAttention(Q,K,V) = softmax(QK^T/âˆšd_k)V
        
        Args:
            Q: Query matrix (batch_size, num_heads, seq_len, d_k)
            K: Key matrix (batch_size, num_heads, seq_len, d_k) 
            V: Value matrix (batch_size, num_heads, seq_len, d_k)
            mask: Optional mask matrix (ç”¨äºå¤„ç†paddingæˆ–å› æœmask)
            
        Returns:
            output: Attention output
            attention_weights: Attention weights
        """
        d_k = Q.shape[-1]
        
        # ã€é¢è¯•è€ƒç‚¹3ã€‘ï¼šæ³¨æ„åŠ›åˆ†æ•°è®¡ç®— - QK^T
        # çŸ©é˜µä¹˜æ³•ï¼š(batch, heads, seq_len, d_k) Ã— (batch, heads, d_k, seq_len)
        # ç»“æœå½¢çŠ¶ï¼š(batch, heads, seq_len, seq_len) - æ¯ä¸ªä½ç½®å¯¹æ‰€æœ‰ä½ç½®çš„æ³¨æ„åŠ›
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        
        # ã€é¢è¯•è€ƒç‚¹4ã€‘ï¼šç¼©æ”¾å› å­âˆšd_kçš„ä½œç”¨
        # é˜²æ­¢softmaxé¥±å’Œï¼šå½“d_kå¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯å€¼ä¼šå¾ˆå¤§ï¼Œå¯¼è‡´softmaxæ¢¯åº¦æ¥è¿‘0
        # é™¤ä»¥âˆšd_kå¯ä»¥æ§åˆ¶ç‚¹ç§¯çš„æ–¹å·®ï¼Œä¿æŒæ¢¯åº¦ç¨³å®š
        
        # ã€é¢è¯•è€ƒç‚¹5ã€‘ï¼šMaskæœºåˆ¶å¤„ç†
        # ç”¨-infæ›¿æ¢maskä½ç½®ï¼Œsoftmaxåè¿™äº›ä½ç½®æƒé‡ä¸º0
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # ã€é¢è¯•è€ƒç‚¹6ã€‘ï¼šSoftmaxå½’ä¸€åŒ–
        # å°†æ³¨æ„åŠ›åˆ†æ•°è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œä¿è¯æƒé‡å’Œä¸º1
        attention_weights = F.softmax(scores, dim=-1)
        
        # ã€é¢è¯•è€ƒç‚¹7ã€‘ï¼šåŠ æƒæ±‚å’Œ
        # ç”¨æ³¨æ„åŠ›æƒé‡å¯¹Valueè¿›è¡ŒåŠ æƒå¹³å‡ï¼Œå¾—åˆ°ä¸Šä¸‹æ–‡å‘é‡
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights
    
    
    def forward(self, x, mask=None):
        """
        Forward pass of Multi-Head Attention
        å¤šå¤´æ³¨æ„åŠ›å‰å‘ä¼ æ’­ - å®Œæ•´çš„è®¡ç®—æµç¨‹
        
        Args:
            x: Input tensor (batch_size, seq_len, d_model)
            mask: Optional attention mask
            
        Returns:
            output: Multi-head attention output
        """
        batch_size, seq_len, d_model = x.shape
        
        # ã€é¢è¯•è€ƒç‚¹9ã€‘ï¼šçº¿æ€§æŠ•å½±ç”ŸæˆQ,K,V
        # å°†è¾“å…¥é€šè¿‡ä¸åŒçš„æƒé‡çŸ©é˜µæŠ•å½±å¾—åˆ°Query, Key, Value
        # è¿™ä½¿å¾—æ¨¡å‹èƒ½å­¦ä¹ åˆ°ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´
        Q = self.W_q(x)  # (batch_size, seq_len, d_model)
        K = self.W_k(x)  # (batch_size, seq_len, d_model)
        V = self.W_v(x)  # (batch_size, seq_len, d_model)
        
        # ã€é¢è¯•è€ƒç‚¹10ã€‘ï¼šå¤šå¤´åˆ†å‰² - æ ¸å¿ƒçš„ç»´åº¦å˜æ¢
        # å°†d_modelç»´åº¦åˆ†å‰²æˆnum_headsä¸ªd_kç»´åº¦çš„å­ç©ºé—´
        # reshape: (batch, seq_len, d_model) -> (batch, seq_len, num_heads, d_k)
        # permute: (batch, seq_len, num_heads, d_k) -> (batch, num_heads, seq_len, d_k)
        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.d_k).permute(0, 2, 1, 3)
        K = K.reshape(batch_size, seq_len, self.num_heads, self.d_k).permute(0, 2, 1, 3)
        V = V.reshape(batch_size, seq_len, self.num_heads, self.d_k).permute(0, 2, 1, 3)
        
        # ã€é¢è¯•è€ƒç‚¹11ã€‘ï¼šå¹¶è¡Œè®¡ç®—å¤šä¸ªæ³¨æ„åŠ›å¤´
        # æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›ï¼Œæ•è·ä¸åŒçš„è¯­ä¹‰å…³ç³»
        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # ã€é¢è¯•è€ƒç‚¹12ã€‘ï¼šå¤šå¤´æ‹¼æ¥ - æ¢å¤åŸå§‹ç»´åº¦
        # permute: (batch, num_heads, seq_len, d_k) -> (batch, seq_len, num_heads, d_k)
        # reshape: (batch, seq_len, num_heads, d_k) -> (batch, seq_len, d_model)
        attention_output = attention_output.permute(0, 2, 1, 3).reshape(
            batch_size, seq_len, self.d_model
        )
        
        # ã€é¢è¯•è€ƒç‚¹13ã€‘ï¼šæœ€ç»ˆçº¿æ€§å˜æ¢
        # é€šè¿‡è¾“å‡ºæŠ•å½±çŸ©é˜µW_oæ•´åˆå¤šå¤´ä¿¡æ¯ï¼Œè¿™æ˜¯å¯å­¦ä¹ çš„å‚æ•°
        output = self.W_o(attention_output)
        
        return output


def test_multi_head_attention():
    """
    Test function for Multi-Head Attention
    ã€é¢è¯•è€ƒç‚¹14ã€‘ï¼šå®Œæ•´çš„æµ‹è¯•ç”¨ä¾‹è®¾è®¡
    """
    print("=== Multi-Head Attention ç®—æ³•å·¥ç¨‹å¸ˆé¢è¯•æµ‹è¯• ===")
    
    # ã€é¢è¯•è€ƒç‚¹15ã€‘ï¼šå…¸å‹çš„è¶…å‚æ•°è®¾ç½®
    batch_size = 2      # æ‰¹å¤§å°
    seq_len = 4         # åºåˆ—é•¿åº¦  
    d_model = 512       # æ¨¡å‹ç»´åº¦ (Transformeræ ‡å‡†é…ç½®)
    num_heads = 8       # æ³¨æ„åŠ›å¤´æ•° (Transformeræ ‡å‡†é…ç½®)
    
    print(f"é…ç½®å‚æ•°: d_model={d_model}, num_heads={num_heads}")
    print(f"æ¯ä¸ªå¤´çš„ç»´åº¦ d_k = d_model // num_heads = {d_model // num_heads}")
    
    # Create test input
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # Initialize multi-head attention
    mha = MultiHeadAttention(d_model, num_heads)
    mha.eval()  # Set to evaluation mode
    
    # ã€é¢è¯•è€ƒç‚¹16ã€‘ï¼šè®¡ç®—å¤æ‚åº¦åˆ†æ
    print(f"\n=== å¤æ‚åº¦åˆ†æ ===")
    print(f"æ—¶é—´å¤æ‚åº¦: O(nÂ²Â·d) å…¶ä¸­n={seq_len}, d={d_model}")
    print(f"ç©ºé—´å¤æ‚åº¦: O(nÂ²Â·h) å…¶ä¸­h={num_heads} (å­˜å‚¨attentionçŸ©é˜µ)")
    
    # Forward pass
    output = mha.forward(x)
    
    print(f"\n=== è¾“å‡ºéªŒè¯ ===")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"å½¢çŠ¶æ˜¯å¦ä¿æŒ: {tuple(output.shape) == tuple(x.shape)}")
    
    # ã€é¢è¯•è€ƒç‚¹17ã€‘ï¼šç»´åº¦æ ¡éªŒ - å…³é”®çš„æ­£ç¡®æ€§æ£€æŸ¥
    assert output.shape == x.shape, f"å½¢çŠ¶ä¸åŒ¹é…! æœŸæœ›{x.shape}, å¾—åˆ°{output.shape}"
    
    # ã€é¢è¯•è€ƒç‚¹18ã€‘ï¼šæ•°å€¼èŒƒå›´æ£€æŸ¥
    print(f"è¾“å‡ºæ•°å€¼èŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")
    print(f"è¾“å‡ºå‡å€¼: {output.mean().item():.4f}, æ ‡å‡†å·®: {output.std().item():.4f}")
    
    # ã€é¢è¯•è€ƒç‚¹19ã€‘ï¼šæ¢¯åº¦æ£€æŸ¥ (ç®€åŒ–ç‰ˆ)
    # åœ¨å®é™…é¢è¯•ä¸­å¯èƒ½ä¼šè¦æ±‚å®ç°åå‘ä¼ æ’­
    print(f"\n=== å…³é”®è®¾è®¡è¦ç‚¹æ€»ç»“ ===")
    print("1. ç»´åº¦å˜æ¢: (batch,seq,d_model) -> (batch,heads,seq,d_k)")
    print("2. ç¼©æ”¾å› å­: 1/âˆšd_k é˜²æ­¢softmaxé¥±å’Œ") 
    print("3. å¹¶è¡Œè®¡ç®—: å¤šå¤´ç‹¬ç«‹è®¡ç®—åæ‹¼æ¥")
    print("4. æ®‹å·®è¿æ¥: å®é™…åº”ç”¨ä¸­éœ€è¦åŠ ä¸Šè¾“å…¥ (æœªåœ¨æ­¤å®ç°)")
    print("5. Layer Norm: é€šå¸¸åœ¨attentionåæ·»åŠ  (æœªåœ¨æ­¤å®ç°)")
    
    print("\nâœ… Multi-Head Attention æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("ğŸ’¡ é¢è¯•é‡ç‚¹: èƒ½è§£é‡Šæ¯ä¸ªæ­¥éª¤çš„æ•°å­¦åŸç†å’Œå·¥ç¨‹å®ç°ç»†èŠ‚")


if __name__ == "__main__":
    test_multi_head_attention()