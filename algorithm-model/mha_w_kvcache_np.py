import numpy as np
import math
from typing import Tuple, Dict, Any
from numpy.typing import NDArray


class MultiHeadAttentionWithKVCache:
    def __init__(self, d_model: int, num_heads: int, max_seq_len: int = 2048) -> None:
        """
        Multi-Head Attention with KV Cache implementation
        å¸¦KVç¼“å­˜çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ - ç”¨äºæ¨ç†åŠ é€Ÿ
        
        Args:
            d_model: dimension of model (æ¨¡å‹ç»´åº¦)
            num_heads: number of attention heads (æ³¨æ„åŠ›å¤´æ•°)
            max_seq_len: maximum sequence length for cache (ç¼“å­˜çš„æœ€å¤§åºåˆ—é•¿åº¦)
        """
        # ã€é¢è¯•è€ƒç‚¹1ã€‘ï¼šç»´åº¦æ ¡éªŒ - d_modelå¿…é¡»èƒ½è¢«num_headsæ•´é™¤
        assert d_model % num_heads == 0, "d_modelå¿…é¡»èƒ½è¢«num_headsæ•´é™¤"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.max_seq_len = max_seq_len
        
        # æƒé‡çŸ©é˜µåˆå§‹åŒ–
        self.W_q: NDArray[np.float64] = np.random.randn(d_model, d_model) * 0.01
        self.W_k: NDArray[np.float64] = np.random.randn(d_model, d_model) * 0.01
        self.W_v: NDArray[np.float64] = np.random.randn(d_model, d_model) * 0.01
        self.W_o: NDArray[np.float64] = np.random.randn(d_model, d_model) * 0.01
        
        # ã€é¢è¯•è€ƒç‚¹2ã€‘ï¼šKV Cacheåˆå§‹åŒ–
        # é¢„åˆ†é…ç¼“å­˜ç©ºé—´ï¼Œé¿å…åŠ¨æ€åˆ†é…å¸¦æ¥çš„æ€§èƒ½å¼€é”€
        self.k_cache = None  # å°†åœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶åˆå§‹åŒ–
        self.v_cache = None
        self.cache_len: int = 0   # å½“å‰ç¼“å­˜çš„åºåˆ—é•¿åº¦
        
    def init_cache(self, batch_size: int) -> None:
        """
        åˆå§‹åŒ–KVç¼“å­˜
        ã€é¢è¯•è€ƒç‚¹3ã€‘ï¼šç¼“å­˜ç©ºé—´é¢„åˆ†é…ç­–ç•¥
        """
        self.k_cache = np.zeros((batch_size, self.num_heads, self.max_seq_len, self.d_k))
        self.v_cache = np.zeros((batch_size, self.num_heads, self.max_seq_len, self.d_k))
        self.cache_len = 0
    
    def update_cache(self, k: NDArray[np.float64], v: NDArray[np.float64], start_pos: int = None) -> None:
        """
        æ›´æ–°KVç¼“å­˜
        ã€é¢è¯•è€ƒç‚¹4ã€‘ï¼šé«˜æ•ˆçš„ç¼“å­˜æ›´æ–°æœºåˆ¶
        
        Args:
            k: Key tensor (batch_size, num_heads, seq_len, d_k)
            v: Value tensor (batch_size, num_heads, seq_len, d_k)
            start_pos: ç¼“å­˜çš„èµ·å§‹ä½ç½®ï¼Œç”¨äºå¢é‡æ›´æ–°
        """
        batch_size, num_heads, seq_len, d_k = k.shape
        
        # å¦‚æœç¼“å­˜æœªåˆå§‹åŒ–ï¼Œå…ˆåˆå§‹åŒ–
        if self.k_cache is None:
            self.init_cache(batch_size)
        
        # ã€é¢è¯•è€ƒç‚¹5ã€‘ï¼šå¢é‡æ›´æ–° vs å…¨é‡æ›´æ–°
        
        if start_pos is None:
            # å…¨é‡æ›´æ–°ï¼šç›´æ¥æ›¿æ¢æ•´ä¸ªç¼“å­˜
            self.k_cache[:, :, :seq_len, :] = k
            self.v_cache[:, :, :seq_len, :] = v
            self.cache_len = seq_len
        else:
            # å¢é‡æ›´æ–°ï¼šåªæ›´æ–°æ–°çš„tokenï¼ˆæ¨ç†æ—¶å¸¸ç”¨ï¼‰
            end_pos = start_pos + seq_len
            self.k_cache[:, :, start_pos:end_pos, :] = k
            self.v_cache[:, :, start_pos:end_pos, :] = v
            self.cache_len = max(self.cache_len, end_pos)
    
    def get_cached_kv(self):
        """
        è·å–ç¼“å­˜çš„K,V
        ã€é¢è¯•è€ƒç‚¹6ã€‘ï¼šç¼“å­˜æ•°æ®çš„é«˜æ•ˆè®¿é—®
        """
        if self.k_cache is None:
            return None, None
        
        # åªè¿”å›æœ‰æ•ˆé•¿åº¦çš„ç¼“å­˜
        k_cached = self.k_cache[:, :, :self.cache_len, :]
        v_cached = self.v_cache[:, :, :self.cache_len, :]
        return k_cached, v_cached
    
    def clear_cache(self) -> None:
        """
        æ¸…ç©ºç¼“å­˜
        ã€é¢è¯•è€ƒç‚¹7ã€‘ï¼šç¼“å­˜ç”Ÿå‘½å‘¨æœŸç®¡ç†
        """
        self.k_cache = None
        self.v_cache = None
        self.cache_len = 0
    
    def scaled_dot_product_attention(self, Q: NDArray[np.float64], K: NDArray[np.float64], V: NDArray[np.float64], mask: NDArray[np.float64] = None) -> Tuple[NDArray[np.float64], NDArray[np.float64]]:
        """
        Scaled Dot-Product Attention with efficient computation
        ä¼˜åŒ–çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›è®¡ç®—
        """
        d_k = Q.shape[-1]
        
        # æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—
        scores = np.matmul(Q, K.transpose(0, 1, 3, 2)) / math.sqrt(d_k)
        
        # Maskå¤„ç†
        if mask is not None:
            scores = np.where(mask == 0, -1e9, scores)
        
        # Softmaxå½’ä¸€åŒ–
        attention_weights = self.softmax(scores)
        
        # åŠ æƒæ±‚å’Œ
        output = np.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def softmax(self, x: NDArray[np.float64]) -> NDArray[np.float64]:
        """æ•°å€¼ç¨³å®šçš„softmaxå®ç°"""
        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)
    
    def forward(self, x: NDArray[np.float64], mask: NDArray[np.float64] = None, use_cache: bool = False, start_pos: int = None) -> NDArray[np.float64]:
        """
        Forward pass with optional KV caching
        æ”¯æŒKVç¼“å­˜çš„å‰å‘ä¼ æ’­
        
        Args:
            x: Input tensor (batch_size, seq_len, d_model)
            mask: Optional attention mask
            use_cache: æ˜¯å¦ä½¿ç”¨KVç¼“å­˜
            start_pos: ç¼“å­˜èµ·å§‹ä½ç½®ï¼ˆå¢é‡æ¨ç†æ—¶ä½¿ç”¨ï¼‰
            
        Returns:
            output: Multi-head attention output
        """
        batch_size, seq_len, d_model = x.shape
        
        # ç”ŸæˆQ,K,V
        Q = np.matmul(x, self.W_q)
        K = np.matmul(x, self.W_k)
        V = np.matmul(x, self.W_v)
        
        # å¤šå¤´åˆ†å‰²
        Q = Q.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)
        K = K.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)
        V = V.reshape(batch_size, seq_len, self.num_heads, self.d_k).transpose(0, 2, 1, 3)
        
        if use_cache:
            # ã€é¢è¯•è€ƒç‚¹8ã€‘ï¼šKVç¼“å­˜çš„ä½¿ç”¨ç­–ç•¥
            if start_pos is not None:
                # å¢é‡æ¨ç†ï¼šæ›´æ–°ç¼“å­˜å¹¶ä½¿ç”¨å†å²K,V
                self.update_cache(K, V, start_pos)
                K_cached, V_cached = self.get_cached_kv()
                if K_cached is not None:
                    K, V = K_cached, V_cached
            else:
                # é¢„å¡«å……é˜¶æ®µï¼šç›´æ¥æ›´æ–°ç¼“å­˜
                self.update_cache(K, V)
        
        # è®¡ç®—æ³¨æ„åŠ›
        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # å¤šå¤´æ‹¼æ¥
        attention_output = attention_output.transpose(0, 2, 1, 3).reshape(
            batch_size, seq_len, self.d_model
        )
        
        # æœ€ç»ˆçº¿æ€§å˜æ¢
        output = np.matmul(attention_output, self.W_o)
        
        return output
    
    def forward_incremental(self, x: NDArray[np.float64], position: int) -> NDArray[np.float64]:
        """
        å¢é‡æ¨ç†çš„å‰å‘ä¼ æ’­
        ã€é¢è¯•è€ƒç‚¹9ã€‘ï¼šé«˜æ•ˆçš„å¢é‡æ¨ç†å®ç°
        
        Args:
            x: å•ä¸ªtokençš„è¾“å…¥ (batch_size, 1, d_model)
            position: å½“å‰tokenåœ¨åºåˆ—ä¸­çš„ä½ç½®
        """
        return self.forward(x, use_cache=True, start_pos=position)
    
    def get_cache_info(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯
        ã€é¢è¯•è€ƒç‚¹10ã€‘ï¼šç¼“å­˜çŠ¶æ€ç›‘æ§
        """
        if self.k_cache is None:
            return {"cache_initialized": False, "cache_len": 0, "max_len": self.max_seq_len}
        
        cache_size_mb = (self.k_cache.nbytes + self.v_cache.nbytes) / (1024 * 1024)
        return {
            "cache_initialized": True,
            "cache_len": self.cache_len,
            "max_len": self.max_seq_len,
            "cache_size_mb": cache_size_mb,
            "utilization": self.cache_len / self.max_seq_len
        }


def test_mha_with_kv_cache() -> None:
    """
    Test function for Multi-Head Attention with KV Cache
    ã€é¢è¯•è€ƒç‚¹11ã€‘ï¼šKVç¼“å­˜åŠŸèƒ½çš„å®Œæ•´æµ‹è¯•
    """
    print("=== Multi-Head Attention with KV Cache æµ‹è¯• ===")
    
    # æµ‹è¯•å‚æ•°
    batch_size = 2
    seq_len = 4
    d_model = 512
    num_heads = 8
    max_seq_len = 1024
    
    print(f"é…ç½®å‚æ•°: d_model={d_model}, num_heads={num_heads}, max_seq_len={max_seq_len}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    mha_cache = MultiHeadAttentionWithKVCache(d_model, num_heads, max_seq_len)
    
    # æµ‹è¯•1ï¼šåŸºç¡€åŠŸèƒ½ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰
    print(f"\n=== æµ‹è¯•1: åŸºç¡€åŠŸèƒ½ ===")
    x = np.random.randn(batch_size, seq_len, d_model)
    output1 = mha_cache.forward(x, use_cache=False)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output1.shape}")
    assert output1.shape == x.shape, "åŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥"
    print("âœ… åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•2ï¼šé¢„å¡«å……é˜¶æ®µï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
    print(f"\n=== æµ‹è¯•2: é¢„å¡«å……é˜¶æ®µ ===")
    mha_cache.clear_cache()
    output2 = mha_cache.forward(x, use_cache=True)
    cache_info = mha_cache.get_cache_info()
    print(f"ç¼“å­˜çŠ¶æ€: {cache_info}")
    print(f"è¾“å‡ºä¸€è‡´æ€§: {np.allclose(output1, output2, rtol=1e-5)}")
    assert output2.shape == x.shape, "é¢„å¡«å……æµ‹è¯•å¤±è´¥"
    print("âœ… é¢„å¡«å……é˜¶æ®µæµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•3ï¼šå¢é‡æ¨ç†
    print(f"\n=== æµ‹è¯•3: å¢é‡æ¨ç† ===")
    # æ¨¡æ‹Ÿæ–°token
    new_token = np.random.randn(batch_size, 1, d_model)
    output_incremental = mha_cache.forward_incremental(new_token, position=seq_len)
    
    # éªŒè¯ç¼“å­˜æ›´æ–°
    cache_info_after = mha_cache.get_cache_info()
    print(f"å¢é‡æ¨ç†åç¼“å­˜é•¿åº¦: {cache_info_after['cache_len']}")
    print(f"é¢„æœŸç¼“å­˜é•¿åº¦: {seq_len + 1}")
    assert cache_info_after['cache_len'] == seq_len + 1, "å¢é‡æ¨ç†ç¼“å­˜æ›´æ–°å¤±è´¥"
    print("âœ… å¢é‡æ¨ç†æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•4ï¼šæ€§èƒ½å¯¹æ¯”æ¨¡æ‹Ÿ
    print(f"\n=== æµ‹è¯•4: æ€§èƒ½åˆ†æ ===")
    long_seq = np.random.randn(1, 100, d_model)
    
    # ä¸ä½¿ç”¨ç¼“å­˜çš„è®¡ç®—
    mha_cache.clear_cache()
    output_no_cache = mha_cache.forward(long_seq, use_cache=False)
    
    # ä½¿ç”¨ç¼“å­˜çš„è®¡ç®—
    mha_cache.clear_cache()
    output_with_cache = mha_cache.forward(long_seq, use_cache=True)
    
    print(f"ç»“æœä¸€è‡´æ€§: {np.allclose(output_no_cache, output_with_cache, rtol=1e-5)}")
    
    # ã€é¢è¯•è€ƒç‚¹12ã€‘ï¼šKVç¼“å­˜çš„ä¼˜åŠ¿åˆ†æ
    print(f"\n=== KVç¼“å­˜ä¼˜åŠ¿åˆ†æ ===")
    print("1. å†…å­˜å¤ç”¨: K,VçŸ©é˜µåªè®¡ç®—ä¸€æ¬¡ï¼Œåç»­æ¨ç†ç›´æ¥ä½¿ç”¨")
    print("2. è®¡ç®—åŠ é€Ÿ: é¿å…é‡å¤è®¡ç®—å†å²tokençš„K,V")
    print("3. åºåˆ—æ‰©å±•: æ”¯æŒå¢é‡æ·»åŠ æ–°tokenè€Œä¸é‡ç®—å…¨åºåˆ—")
    print("4. å†…å­˜æ•ˆç‡: é¢„åˆ†é…å›ºå®šå¤§å°é¿å…åŠ¨æ€æ‰©å®¹")
    
    # ã€é¢è¯•è€ƒç‚¹13ã€‘ï¼šå®é™…åº”ç”¨åœºæ™¯
    print(f"\n=== å®é™…åº”ç”¨åœºæ™¯ ===")
    print("1. æ–‡æœ¬ç”Ÿæˆ: GPTç±»æ¨¡å‹çš„è‡ªå›å½’ç”Ÿæˆ")
    print("2. å¯¹è¯ç³»ç»Ÿ: å¤šè½®å¯¹è¯çš„ä¸Šä¸‹æ–‡ä¿æŒ")
    print("3. ä»£ç è¡¥å…¨: IDEä¸­çš„å®æ—¶ä»£ç å»ºè®®")
    print("4. æœºå™¨ç¿»è¯‘: é•¿æ–‡æœ¬çš„é€æ­¥ç¿»è¯‘")
    
    print(f"\nâœ… Multi-Head Attention with KV Cache æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("ğŸ’¡ é¢è¯•é‡ç‚¹: KVç¼“å­˜æ˜¯æ¨ç†ä¼˜åŒ–çš„å…³é”®æŠ€æœ¯ï¼Œèƒ½æ˜¾è‘—æå‡ç”Ÿæˆä»»åŠ¡çš„æ•ˆç‡")


if __name__ == "__main__":
    test_mha_with_kv_cache()