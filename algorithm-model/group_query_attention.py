import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class GroupQueryAttention(nn.Module):
    def __init__(self, d_model, num_query_heads, num_kv_heads):
        """
        Group Query Attention implementation
        åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶å®ç° - é«˜æ•ˆæ¨ç†çš„Transformerç»„ä»¶
        
        Args:
            d_model: dimension of model (æ¨¡å‹ç»´åº¦)
            num_query_heads: number of query heads (æŸ¥è¯¢å¤´æ•°)
            num_kv_heads: number of key-value heads (é”®å€¼å¤´æ•°)
        """
        super(GroupQueryAttention, self).__init__()
        
        # ã€é¢è¯•è€ƒç‚¹1ã€‘ï¼šGQAæ ¸å¿ƒçº¦æŸ - Queryå¤´æ•°å¿…é¡»æ˜¯KVå¤´æ•°çš„æ•´æ•°å€
        # è¿™æ ·æ‰èƒ½å°†Queryå¤´å‡åŒ€åˆ†ç»„ï¼Œæ¯ç»„å…±äº«ä¸€ä¸ªKVå¤´
        assert num_query_heads % num_kv_heads == 0, "num_query_headså¿…é¡»æ˜¯num_kv_headsçš„æ•´æ•°å€"
        assert d_model % num_query_heads == 0, "d_modelå¿…é¡»èƒ½è¢«num_query_headsæ•´é™¤"
        
        self.d_model = d_model
        self.num_query_heads = num_query_heads  # Queryå¤´æ•°ï¼ˆé€šå¸¸è¾ƒå¤šï¼‰
        self.num_kv_heads = num_kv_heads        # KVå¤´æ•°ï¼ˆé€šå¸¸è¾ƒå°‘ï¼‰
        self.d_k = d_model // num_query_heads   # æ¯ä¸ªQueryå¤´çš„ç»´åº¦
        self.d_kv = d_model // num_kv_heads     # æ¯ä¸ªKVå¤´çš„ç»´åº¦
        
        # ã€é¢è¯•è€ƒç‚¹2ã€‘ï¼šåˆ†ç»„æ¯”ä¾‹è®¡ç®—
        # group_sizeè¡¨ç¤ºå¤šå°‘ä¸ªQueryå¤´å…±äº«ä¸€ä¸ªKVå¤´
        self.group_size = num_query_heads // num_kv_heads
        
        # ã€é¢è¯•è€ƒç‚¹3ã€‘ï¼šå‚æ•°é‡å¯¹æ¯”åˆ†æ
        # QueryæŠ•å½±ï¼šä¿æŒåŸæœ‰ç»´åº¦ (d_model -> d_model)
        # KVæŠ•å½±ï¼šç»´åº¦å‡å°‘ (d_model -> num_kv_heads * d_kv)
        self.W_q = nn.Linear(d_model, d_model, bias=False)
        self.W_k = nn.Linear(d_model, num_kv_heads * self.d_kv, bias=False)
        self.W_v = nn.Linear(d_model, num_kv_heads * self.d_kv, bias=False)
        self.W_o = nn.Linear(d_model, d_model, bias=False)
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        """
        Scaled Dot-Product Attention for GQA
        GQAç‰ˆæœ¬çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼šå¤„ç†ä¸åŒå¤´æ•°çš„Qå’ŒKV
        
        Args:
            Q: Query matrix (batch_size, num_query_heads, seq_len, d_k)
            K: Key matrix (batch_size, num_kv_heads, seq_len, d_kv) 
            V: Value matrix (batch_size, num_kv_heads, seq_len, d_kv)
            mask: Optional mask matrix
            
        Returns:
            output: Attention output
            attention_weights: Attention weights
        """
        d_k = Q.shape[-1]
        
        # ã€é¢è¯•è€ƒç‚¹4ã€‘ï¼šKVå¤´å¤åˆ¶ç­–ç•¥
        # å°†æ¯ä¸ªKVå¤´å¤åˆ¶group_sizeæ¬¡ï¼Œä½¿å…¶åŒ¹é…Queryå¤´æ•°
        # repeat_interleaveä¿è¯åˆ†ç»„å¯¹åº”å…³ç³»æ­£ç¡®
        K = K.repeat_interleave(self.group_size, dim=1)  # (batch, num_query_heads, seq_len, d_kv)
        V = V.repeat_interleave(self.group_size, dim=1)  # (batch, num_query_heads, seq_len, d_kv)

        """
        interleave = äº¤é”™ã€ç©¿æ’
        - åƒç¼–ç»‡ä¸€æ ·ï¼Œå°†å…ƒç´ äº¤æ›¿æ’åˆ—

        repeat_interleave = å°±åœ°é‡å¤æ¯ä¸ªå…ƒç´ 
        - [1,2,3].repeat_interleave(2) â†’ [1,1,2,2,3,3]
        - [1,2,3].repeat(2) â†’ [1,2,3,1,2,3]
        """
        
        # ã€é¢è¯•è€ƒç‚¹5ã€‘ï¼šç»´åº¦ä¸€è‡´æ€§æ£€æŸ¥
        # ç¡®ä¿Qå’Œæ‰©å±•åçš„K,Våœ¨å¤´æ•°ç»´åº¦ä¸ŠåŒ¹é…
        assert Q.shape[1] == K.shape[1] == V.shape[1], "å¤´æ•°ç»´åº¦ä¸åŒ¹é…"
        
        # æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—ï¼šQK^T / âˆšd_k
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        
        # Maskå¤„ç†
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # Softmaxå½’ä¸€åŒ–
        attention_weights = F.softmax(scores, dim=-1)
        
        # åŠ æƒæ±‚å’Œ
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights
    
    
    def forward(self, x, mask=None):
        """
        Forward pass of Group Query Attention
        åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›å‰å‘ä¼ æ’­
        
        Args:
            x: Input tensor (batch_size, seq_len, d_model)
            mask: Optional attention mask
            
        Returns:
            output: Group query attention output
        """
        batch_size, seq_len, d_model = x.shape
        
        # ã€é¢è¯•è€ƒç‚¹6ã€‘ï¼šä¸å¯¹ç§°çš„çº¿æ€§æŠ•å½±
        # Queryç»´æŒåŸæœ‰å¤æ‚åº¦ï¼ŒKVæŠ•å½±ç»´åº¦å‡å°‘
        Q = self.W_q(x)  # (batch_size, seq_len, d_model)
        K = self.W_k(x)  # (batch_size, seq_len, num_kv_heads * d_kv)
        V = self.W_v(x)  # (batch_size, seq_len, num_kv_heads * d_kv)
        
        # ã€é¢è¯•è€ƒç‚¹7ã€‘ï¼šåˆ†ç»„é‡å¡‘ç­–ç•¥
        # Query: é‡å¡‘ä¸ºæ ‡å‡†å¤šå¤´æ ¼å¼
        Q = Q.reshape(batch_size, seq_len, self.num_query_heads, self.d_k).permute(0, 2, 1, 3)
        
        # KV: é‡å¡‘ä¸ºè¾ƒå°‘çš„å¤´æ•°æ ¼å¼
        K = K.reshape(batch_size, seq_len, self.num_kv_heads, self.d_kv).permute(0, 2, 1, 3)
        V = V.reshape(batch_size, seq_len, self.num_kv_heads, self.d_kv).permute(0, 2, 1, 3)
        
        # ã€é¢è¯•è€ƒç‚¹8ã€‘ï¼šåˆ†ç»„æ³¨æ„åŠ›è®¡ç®—
        # é€šè¿‡KVå¤´å¤åˆ¶å®ç°åˆ†ç»„å…±äº«æœºåˆ¶
        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # ã€é¢è¯•è€ƒç‚¹9ã€‘ï¼šè¾“å‡ºæ‹¼æ¥æ¢å¤
        # å°†å¤šå¤´è¾“å‡ºæ‹¼æ¥å›åŸå§‹ç»´åº¦
        attention_output = attention_output.permute(0, 2, 1, 3).reshape(
            batch_size, seq_len, self.d_model
        )
        
        # æœ€ç»ˆçº¿æ€§å˜æ¢
        output = self.W_o(attention_output)
        
        return output


def test_group_query_attention():
    """
    Test function for Group Query Attention
    ã€é¢è¯•è€ƒç‚¹10ã€‘ï¼šGQA vs MHA å¯¹æ¯”æµ‹è¯•
    """
    print("=== Group Query Attention ç®—æ³•å·¥ç¨‹å¸ˆé¢è¯•æµ‹è¯• ===")
    
    # ã€é¢è¯•è€ƒç‚¹11ã€‘ï¼šå…¸å‹çš„GQAé…ç½®
    batch_size = 2
    seq_len = 4
    d_model = 512
    num_query_heads = 32    # Queryå¤´æ•°è¾ƒå¤š
    num_kv_heads = 8        # KVå¤´æ•°è¾ƒå°‘ï¼Œ4:1åˆ†ç»„æ¯”ä¾‹
    
    print(f"é…ç½®å‚æ•°: d_model={d_model}")
    print(f"Queryå¤´æ•°: {num_query_heads}, KVå¤´æ•°: {num_kv_heads}")
    print(f"åˆ†ç»„å¤§å°: {num_query_heads // num_kv_heads} (æ¯{num_query_heads // num_kv_heads}ä¸ªQueryå¤´å…±äº«1ä¸ªKVå¤´)")
    
    # Create test input
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # Initialize group query attention
    gqa = GroupQueryAttention(d_model, num_query_heads, num_kv_heads)
    gqa.eval()
    
    # ã€é¢è¯•è€ƒç‚¹12ã€‘ï¼šå‚æ•°é‡å¯¹æ¯”åˆ†æ
    def count_parameters(model):
        return sum(p.numel() for p in model.parameters())
    
    gqa_params = count_parameters(gqa)
    
    # å¯¹æ¯”æ ‡å‡†MHAçš„å‚æ•°é‡
    from multi_head_attention import MultiHeadAttention
    mha = MultiHeadAttention(d_model, num_query_heads)
    mha_params = count_parameters(mha)
    
    print(f"\n=== å‚æ•°é‡å¯¹æ¯” ===")
    print(f"GQAå‚æ•°é‡: {gqa_params:,}")
    print(f"MHAå‚æ•°é‡: {mha_params:,}")
    print(f"å‚æ•°å‡å°‘: {((mha_params - gqa_params) / mha_params * 100):.1f}%")
    
    # ã€é¢è¯•è€ƒç‚¹13ã€‘ï¼šè®¡ç®—å¤æ‚åº¦åˆ†æ
    print(f"\n=== å¤æ‚åº¦åˆ†æ ===")
    print(f"KVæŠ•å½±å¤æ‚åº¦: O(dÂ·nÂ·h_kv) å…¶ä¸­h_kv={num_kv_heads}")
    print(f"æ³¨æ„åŠ›è®¡ç®—: O(nÂ²Â·h_q) å…¶ä¸­h_q={num_query_heads}")
    print(f"KVç¼“å­˜å¤§å°: {num_kv_heads}/{num_query_heads} = {num_kv_heads/num_query_heads:.2f}å€ (ç›¸æ¯”MHA)")
    
    # Forward pass
    output = gqa.forward(x)
    
    print(f"\n=== è¾“å‡ºéªŒè¯ ===")
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"å½¢çŠ¶æ˜¯å¦ä¿æŒ: {tuple(output.shape) == tuple(x.shape)}")
    
    # ã€é¢è¯•è€ƒç‚¹14ã€‘ï¼šæ­£ç¡®æ€§æ£€æŸ¥
    assert output.shape == x.shape, f"å½¢çŠ¶ä¸åŒ¹é…! æœŸæœ›{x.shape}, å¾—åˆ°{output.shape}"
    
    print(f"è¾“å‡ºæ•°å€¼èŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")
    print(f"è¾“å‡ºå‡å€¼: {output.mean().item():.4f}, æ ‡å‡†å·®: {output.std().item():.4f}")
    
    # ã€é¢è¯•è€ƒç‚¹15ã€‘ï¼šGQAå…³é”®ä¼˜åŠ¿æ€»ç»“
    print(f"\n=== GQAæ ¸å¿ƒä¼˜åŠ¿ ===")
    print("1. å‚æ•°æ•ˆç‡: KVæŠ•å½±å‚æ•°å‡å°‘ï¼Œé™ä½æ¨¡å‹å¤§å°")
    print("2. æ¨ç†åŠ é€Ÿ: KVç¼“å­˜å¤§å°å‡å°‘ï¼Œæå‡é•¿åºåˆ—æ¨ç†é€Ÿåº¦")
    print("3. è´¨é‡ä¿æŒ: Queryå¤´æ•°ä¸å˜ï¼Œä¿æŒè¡¨è¾¾èƒ½åŠ›")
    print("4. å†…å­˜ä¼˜åŒ–: æ¨ç†æ—¶KVç¼“å­˜å†…å­˜å ç”¨æ˜¾è‘—é™ä½")
    print("5. åˆ†ç»„è®¾è®¡: å¤šä¸ªQueryå¤´å…±äº«KVå¤´ï¼Œå¹³è¡¡æ•ˆç‡ä¸æ€§èƒ½")
    
    # ã€é¢è¯•è€ƒç‚¹16ã€‘ï¼šé€‚ç”¨åœºæ™¯åˆ†æ
    print(f"\n=== åº”ç”¨åœºæ™¯ ===")
    print("â€¢ å¤§æ¨¡å‹æ¨ç†ä¼˜åŒ– (å¦‚LLaMA-2, Code Llama)")
    print("â€¢ é•¿åºåˆ—ç”Ÿæˆä»»åŠ¡ (å‡å°‘KVç¼“å­˜å‹åŠ›)")  
    print("â€¢ èµ„æºå—é™ç¯å¢ƒ (ç§»åŠ¨ç«¯ã€è¾¹ç¼˜è®¡ç®—)")
    print("â€¢ å®æ—¶å¯¹è¯ç³»ç»Ÿ (é™ä½æ¨ç†å»¶è¿Ÿ)")
    
    print("\nâœ… Group Query Attention æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("ğŸ’¡ é¢è¯•é‡ç‚¹: ç†è§£GQAçš„åˆ†ç»„æœºåˆ¶å’Œæ•ˆç‡ä¼˜åŠ¿")


if __name__ == "__main__":
    test_group_query_attention()