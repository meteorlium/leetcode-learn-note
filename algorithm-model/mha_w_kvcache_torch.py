import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from typing import Tuple, Dict, Any


class MultiHeadAttentionWithKVCache(nn.Module):
    def __init__(self, d_model: int, num_heads: int, max_seq_len: int = 2048, device: str = "cpu"):
        """
        Multi-Head Attention with KV Cache implementation (PyTorchç‰ˆæœ¬)
        å¸¦KVç¼“å­˜çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ - ç”¨äºæ¨ç†åŠ é€Ÿ
        
        Args:
            d_model: dimension of model (æ¨¡å‹ç»´åº¦)
            num_heads: number of attention heads (æ³¨æ„åŠ›å¤´æ•°)
            max_seq_len: maximum sequence length for cache (ç¼“å­˜çš„æœ€å¤§åºåˆ—é•¿åº¦)
            device: è®¡ç®—è®¾å¤‡ ("cpu" æˆ– "cuda")
        """
        super().__init__()
        
        # ã€é¢è¯•è€ƒç‚¹1ã€‘ï¼šç»´åº¦æ ¡éªŒ - d_modelå¿…é¡»èƒ½è¢«num_headsæ•´é™¤
        assert d_model % num_heads == 0, "d_modelå¿…é¡»èƒ½è¢«num_headsæ•´é™¤"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads
        self.max_seq_len = max_seq_len
        self.device = device
        
        # ã€é¢è¯•è€ƒç‚¹2ã€‘ï¼šPyTorch nn.Linearå±‚çš„ä¼˜åŠ¿
        # ç›¸æ¯”æ‰‹åŠ¨æƒé‡çŸ©é˜µï¼Œnn.Linearæä¾›æ›´å¥½çš„åˆå§‹åŒ–å’ŒGPUåŠ é€Ÿ
        self.W_q = nn.Linear(d_model, d_model, bias=False)
        self.W_k = nn.Linear(d_model, d_model, bias=False)
        self.W_v = nn.Linear(d_model, d_model, bias=False)
        self.W_o = nn.Linear(d_model, d_model, bias=False)
        
        # ã€é¢è¯•è€ƒç‚¹3ã€‘ï¼šKV Cacheåˆå§‹åŒ– - ä½¿ç”¨register_buffer
        # register_buffer: æ³¨å†Œéå‚æ•°å¼ é‡ï¼Œè‡ªåŠ¨è·Ÿéšæ¨¡å‹è®¾å¤‡å˜åŒ–ï¼Œä¿å­˜åœ¨state_dictä¸­ï¼Œä½†ä¸å‚ä¸æ¢¯åº¦è®¡ç®—
        self.register_buffer('k_cache', torch.empty(0))
        self.register_buffer('v_cache', torch.empty(0))
        self.cache_len = 0
        
        # ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡
        self.to(device)
        
    def init_cache(self, batch_size: int) -> None:
        """
        åˆå§‹åŒ–KVç¼“å­˜
        ã€é¢è¯•è€ƒç‚¹4ã€‘ï¼šPyTorchç¼“å­˜ç©ºé—´é¢„åˆ†é…ç­–ç•¥
        """
        # ä½¿ç”¨torch.zerosåœ¨æŒ‡å®šè®¾å¤‡ä¸Šåˆ›å»ºç¼“å­˜
        self.k_cache = torch.zeros(
            batch_size, self.num_heads, self.max_seq_len, self.d_k,
            device=self.device, dtype=torch.float32
        )
        self.v_cache = torch.zeros(
            batch_size, self.num_heads, self.max_seq_len, self.d_k,
            device=self.device, dtype=torch.float32
        )
        self.cache_len = 0
    
    def update_cache(self, k: torch.Tensor, v: torch.Tensor, start_pos: int = None) -> None:
        """
        æ›´æ–°KVç¼“å­˜
        ã€é¢è¯•è€ƒç‚¹5ã€‘ï¼šé«˜æ•ˆçš„PyTorchç¼“å­˜æ›´æ–°æœºåˆ¶
        
        Args:
            k: Key tensor (batch_size, num_heads, seq_len, d_k)
            v: Value tensor (batch_size, num_heads, seq_len, d_k)
            start_pos: ç¼“å­˜çš„èµ·å§‹ä½ç½®ï¼Œç”¨äºå¢é‡æ›´æ–°
        """
        batch_size, num_heads, seq_len, d_k = k.shape
        
        # å¦‚æœç¼“å­˜æœªåˆå§‹åŒ–ï¼Œå…ˆåˆå§‹åŒ–
        # numel() = number of elementsï¼Œæ£€æŸ¥å¼ é‡å…ƒç´ æ€»æ•°æ˜¯å¦ä¸º0ï¼ˆå³ç©ºå¼ é‡ï¼‰
        if self.k_cache.numel() == 0:
            self.init_cache(batch_size)
        
        # ã€é¢è¯•è€ƒç‚¹6ã€‘ï¼šPyTorchå¼ é‡åˆ‡ç‰‡èµ‹å€¼çš„é«˜æ•ˆæ€§
        if start_pos is None:
            # å…¨é‡æ›´æ–°ï¼šç›´æ¥æ›¿æ¢æ•´ä¸ªç¼“å­˜
            self.k_cache[:, :, :seq_len, :] = k
            self.v_cache[:, :, :seq_len, :] = v
            self.cache_len = seq_len
        else:
            # å¢é‡æ›´æ–°ï¼šåªæ›´æ–°æ–°çš„tokenï¼ˆæ¨ç†æ—¶å¸¸ç”¨ï¼‰
            end_pos = start_pos + seq_len
            self.k_cache[:, :, start_pos:end_pos, :] = k
            self.v_cache[:, :, start_pos:end_pos, :] = v
            self.cache_len = max(self.cache_len, end_pos)
    
    def get_cached_kv(self) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        è·å–ç¼“å­˜çš„K,V
        ã€é¢è¯•è€ƒç‚¹7ã€‘ï¼šPyTorchå¼ é‡åˆ‡ç‰‡çš„å†…å­˜æ•ˆç‡
        """
        if self.k_cache.numel() == 0:
            return None, None
        
        # åªè¿”å›æœ‰æ•ˆé•¿åº¦çš„ç¼“å­˜
        k_cached = self.k_cache[:, :, :self.cache_len, :]
        v_cached = self.v_cache[:, :, :self.cache_len, :]
        return k_cached, v_cached
    
    def clear_cache(self) -> None:
        """
        æ¸…ç©ºç¼“å­˜
        ã€é¢è¯•è€ƒç‚¹8ã€‘ï¼šPyTorchç¼“å­˜ç”Ÿå‘½å‘¨æœŸç®¡ç†
        """
        # é‡ç½®ä¸ºç©ºå¼ é‡è€Œä¸æ˜¯Noneï¼Œä¿æŒè®¾å¤‡ä¸€è‡´æ€§
        # torch.empty(0): åˆ›å»ºæœªåˆå§‹åŒ–çš„ç©ºå¼ é‡ï¼Œæ¯”zerosæ›´å¿«ï¼Œä¿æŒtensorç±»å‹
        self.k_cache = torch.empty(0, device=self.device)
        self.v_cache = torch.empty(0, device=self.device)
        self.cache_len = 0
    
    def scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, 
                                   mask: torch.Tensor = None) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Scaled Dot-Product Attention with PyTorch optimizations
        ä¼˜åŒ–çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›è®¡ç®—
        
        ã€é¢è¯•è€ƒç‚¹9ã€‘ï¼šPyTorch vs NumPyçš„æ€§èƒ½ä¼˜åŠ¿
        - è‡ªåŠ¨GPUåŠ é€Ÿ
        - ä¼˜åŒ–çš„BLASåº“è°ƒç”¨
        - å†…å­˜å¸ƒå±€ä¼˜åŒ–
        """
        d_k = Q.size(-1)
        
        # ã€é¢è¯•è€ƒç‚¹10ã€‘ï¼štorch.matmulçš„å¹¿æ’­å’Œä¼˜åŒ–
        # PyTorchçš„matmulé’ˆå¯¹æ‰¹å¤„ç†çŸ©é˜µä¹˜æ³•è¿›è¡Œäº†é«˜åº¦ä¼˜åŒ–
        # math.sqrt(d_k): d_kæ˜¯å¸¸æ•°ï¼Œç”¨mathæ¯”torch.sqrtæ›´é«˜æ•ˆï¼Œé¿å…åˆ›å»ºä¸å¿…è¦çš„tensor
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        
        # Maskå¤„ç† - ä½¿ç”¨PyTorchçš„masked_fill
        # masked_fill(mask, value): å°†maskä¸ºTrueçš„ä½ç½®å¡«å……ä¸ºvalueï¼Œç”¨äºå±è”½paddingä½ç½®
        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # ã€é¢è¯•è€ƒç‚¹11ã€‘ï¼šF.softmax vs æ‰‹åŠ¨å®ç°
        # PyTorchçš„softmaxä½¿ç”¨äº†æ•°å€¼ç¨³å®šçš„å®ç°å’ŒGPUä¼˜åŒ–
        attention_weights = F.softmax(scores, dim=-1)
        
        # åŠ æƒæ±‚å’Œ
        output = torch.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def forward(self, x: torch.Tensor, mask: torch.Tensor = None, 
                use_cache: bool = False, start_pos: int = None) -> torch.Tensor:
        """
        Forward pass with optional KV caching
        æ”¯æŒKVç¼“å­˜çš„å‰å‘ä¼ æ’­
        
        Args:
            x: Input tensor (batch_size, seq_len, d_model)
            mask: Optional attention mask
            use_cache: æ˜¯å¦ä½¿ç”¨KVç¼“å­˜
            start_pos: ç¼“å­˜èµ·å§‹ä½ç½®ï¼ˆå¢é‡æ¨ç†æ—¶ä½¿ç”¨ï¼‰
            
        Returns:
            output: Multi-head attention output
        """
        batch_size, seq_len, d_model = x.shape
        
        # ã€é¢è¯•è€ƒç‚¹12ã€‘ï¼šPyTorch nn.Linearçš„å‰å‘ä¼ æ’­
        # è‡ªåŠ¨å¤„ç†æ‰¹å¤„ç†ç»´åº¦ï¼Œæ”¯æŒGPUåŠ é€Ÿ
        Q = self.W_q(x)  # (batch_size, seq_len, d_model)
        K = self.W_k(x)  # (batch_size, seq_len, d_model)
        V = self.W_v(x)  # (batch_size, seq_len, d_model)
        
        # ã€é¢è¯•è€ƒç‚¹13ã€‘ï¼šPyTorch view vs reshapeçš„åŒºåˆ«
        # viewè¦æ±‚è¿ç»­å†…å­˜ï¼Œreshapeä¼šåœ¨éœ€è¦æ—¶å¤åˆ¶æ•°æ®
        Q = Q.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        K = K.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        V = V.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2)
        
        if use_cache:
            # ã€é¢è¯•è€ƒç‚¹14ã€‘ï¼šKVç¼“å­˜çš„ä½¿ç”¨ç­–ç•¥
            if start_pos is not None:
                # å¢é‡æ¨ç†ï¼šæ›´æ–°ç¼“å­˜å¹¶ä½¿ç”¨å†å²K,V
                self.update_cache(K, V, start_pos)
                K_cached, V_cached = self.get_cached_kv()
                if K_cached is not None:
                    K, V = K_cached, V_cached
            else:
                # é¢„å¡«å……é˜¶æ®µï¼šç›´æ¥æ›´æ–°ç¼“å­˜
                self.update_cache(K, V)
        
        # è®¡ç®—æ³¨æ„åŠ›
        attention_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)
        
        # ã€é¢è¯•è€ƒç‚¹15ã€‘ï¼šå¤šå¤´æ‹¼æ¥çš„å†…å­˜å¸ƒå±€ä¼˜åŒ–
        attention_output = attention_output.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.d_model
        )
        
        # æœ€ç»ˆçº¿æ€§å˜æ¢
        output = self.W_o(attention_output)
        
        return output
    
    def forward_incremental(self, x: torch.Tensor, position: int) -> torch.Tensor:
        """
        å¢é‡æ¨ç†çš„å‰å‘ä¼ æ’­
        ã€é¢è¯•è€ƒç‚¹16ã€‘ï¼šé«˜æ•ˆçš„å¢é‡æ¨ç†å®ç°
        
        Args:
            x: å•ä¸ªtokençš„è¾“å…¥ (batch_size, 1, d_model)
            position: å½“å‰tokenåœ¨åºåˆ—ä¸­çš„ä½ç½®
        """
        return self.forward(x, use_cache=True, start_pos=position)
    
    @torch.no_grad()
    def generate_text(self, prompt_tokens: torch.Tensor, max_new_tokens: int = 50) -> torch.Tensor:
        """
        æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹
        ã€é¢è¯•è€ƒç‚¹17ã€‘ï¼šå®é™…åº”ç”¨ä¸­çš„è‡ªå›å½’ç”Ÿæˆ
        
        Args:
            prompt_tokens: æç¤ºè¯tokenåºåˆ— (batch_size, prompt_len, d_model)
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Returns:
            generated_tokens: ç”Ÿæˆçš„å®Œæ•´åºåˆ—
        """
        batch_size, prompt_len, d_model = prompt_tokens.shape
        
        # é¢„å¡«å……é˜¶æ®µï¼šå¤„ç†æ•´ä¸ªprompt
        self.clear_cache()
        _ = self.forward(prompt_tokens, use_cache=True)
        
        generated = prompt_tokens
        
        # å¢é‡ç”Ÿæˆé˜¶æ®µï¼šé€ä¸ªç”Ÿæˆæ–°token
        for i in range(max_new_tokens):
            # æ¨¡æ‹Ÿä¸‹ä¸€ä¸ªtokenï¼ˆå®é™…åº”ç”¨ä¸­è¿™é‡Œä¼šæ˜¯è¯­è¨€æ¨¡å‹çš„è¾“å‡ºï¼‰
            next_token = torch.randn(batch_size, 1, d_model, device=self.device)
            
            # å¢é‡æ¨ç†
            _ = self.forward_incremental(next_token, prompt_len + i)
            
            # æ‹¼æ¥åˆ°ç”Ÿæˆåºåˆ—
            generated = torch.cat([generated, next_token], dim=1)
        
        return generated
    
    def get_cache_info(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜çŠ¶æ€ä¿¡æ¯
        ã€é¢è¯•è€ƒç‚¹18ã€‘ï¼šPyTorchå¼ é‡å†…å­˜ç›‘æ§
        """
        if self.k_cache.numel() == 0:
            return {
                "cache_initialized": False,
                "cache_len": 0,
                "max_len": self.max_seq_len,
                "device": self.device
            }
        
        # PyTorchå¼ é‡å†…å­˜å¤§å°è®¡ç®—
        cache_size_mb = (self.k_cache.numel() + self.v_cache.numel()) * 4 / (1024 * 1024)  # float32 = 4 bytes
        
        return {
            "cache_initialized": True,
            "cache_len": self.cache_len,
            "max_len": self.max_seq_len,
            "cache_size_mb": cache_size_mb,
            "utilization": self.cache_len / self.max_seq_len,
            "device": self.device,
            "cache_shape": self.k_cache.shape
        }


def test_mha_with_kv_cache_torch():
    """
    Test function for PyTorch Multi-Head Attention with KV Cache
    ã€é¢è¯•è€ƒç‚¹19ã€‘ï¼šPyTorchç‰ˆæœ¬çš„å®Œæ•´æµ‹è¯•
    """
    print("=== PyTorch Multi-Head Attention with KV Cache æµ‹è¯• ===")
    
    # æ£€æµ‹è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æµ‹è¯•å‚æ•°
    batch_size = 2
    seq_len = 4
    d_model = 512
    num_heads = 8
    max_seq_len = 1024
    
    print(f"é…ç½®å‚æ•°: d_model={d_model}, num_heads={num_heads}, max_seq_len={max_seq_len}")
    
    # åˆå§‹åŒ–æ¨¡å‹
    mha_cache = MultiHeadAttentionWithKVCache(d_model, num_heads, max_seq_len, device)
    
    # æµ‹è¯•1ï¼šåŸºç¡€åŠŸèƒ½ï¼ˆä¸ä½¿ç”¨ç¼“å­˜ï¼‰
    print(f"\n=== æµ‹è¯•1: åŸºç¡€åŠŸèƒ½ ===")
    x = torch.randn(batch_size, seq_len, d_model, device=device)
    output1 = mha_cache(x, use_cache=False)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}, è®¾å¤‡: {x.device}")
    print(f"è¾“å‡ºå½¢çŠ¶: {output1.shape}, è®¾å¤‡: {output1.device}")
    assert output1.shape == x.shape, "åŸºç¡€åŠŸèƒ½æµ‹è¯•å¤±è´¥"
    print("âœ… åŸºç¡€åŠŸèƒ½æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•2ï¼šé¢„å¡«å……é˜¶æ®µï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰
    print(f"\n=== æµ‹è¯•2: é¢„å¡«å……é˜¶æ®µ ===")
    mha_cache.clear_cache()
    output2 = mha_cache(x, use_cache=True)
    cache_info = mha_cache.get_cache_info()
    print(f"ç¼“å­˜çŠ¶æ€: {cache_info}")
    print(f"è¾“å‡ºä¸€è‡´æ€§: {torch.allclose(output1, output2, rtol=1e-5)}")
    assert output2.shape == x.shape, "é¢„å¡«å……æµ‹è¯•å¤±è´¥"
    print("âœ… é¢„å¡«å……é˜¶æ®µæµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•3ï¼šå¢é‡æ¨ç†
    print(f"\n=== æµ‹è¯•3: å¢é‡æ¨ç† ===")
    new_token = torch.randn(batch_size, 1, d_model, device=device)
    output_incremental = mha_cache.forward_incremental(new_token, position=seq_len)
    
    # éªŒè¯ç¼“å­˜æ›´æ–°
    cache_info_after = mha_cache.get_cache_info()
    print(f"å¢é‡æ¨ç†åç¼“å­˜é•¿åº¦: {cache_info_after['cache_len']}")
    print(f"é¢„æœŸç¼“å­˜é•¿åº¦: {seq_len + 1}")
    assert cache_info_after['cache_len'] == seq_len + 1, "å¢é‡æ¨ç†ç¼“å­˜æ›´æ–°å¤±è´¥"
    print("âœ… å¢é‡æ¨ç†æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•4ï¼šæ–‡æœ¬ç”Ÿæˆç¤ºä¾‹
    print(f"\n=== æµ‹è¯•4: æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹ ===")
    prompt = torch.randn(1, 3, d_model, device=device)  # 3ä¸ªtokençš„prompt
    generated = mha_cache.generate_text(prompt, max_new_tokens=5)
    print(f"Prompté•¿åº¦: {prompt.shape[1]}")
    print(f"ç”Ÿæˆåºåˆ—é•¿åº¦: {generated.shape[1]}")
    print(f"é¢„æœŸé•¿åº¦: {3 + 5}")
    assert generated.shape[1] == 8, "æ–‡æœ¬ç”Ÿæˆæµ‹è¯•å¤±è´¥"
    print("âœ… æ–‡æœ¬ç”Ÿæˆæµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•5ï¼šMaskåŠŸèƒ½æµ‹è¯•
    print(f"\n=== æµ‹è¯•5: MaskåŠŸèƒ½æµ‹è¯• ===")
    
    # åˆ›å»ºpadding maskç¤ºä¾‹
    # å‡è®¾ç¬¬1ä¸ªåºåˆ—æœ‰3ä¸ªæœ‰æ•ˆtokenï¼Œç¬¬2ä¸ªåºåˆ—æœ‰2ä¸ªæœ‰æ•ˆtoken
    padding_mask = torch.tensor([[1, 1, 1, 0],   # ç¬¬1ä¸ªåºåˆ—ï¼šå‰3ä¸ªæœ‰æ•ˆ
                                [1, 1, 0, 0]], device=device, dtype=torch.float32)  # ç¬¬2ä¸ªåºåˆ—ï¼šå‰2ä¸ªæœ‰æ•ˆ
    
    # æ‰©å±•åˆ°attentionç»´åº¦ (batch_size, seq_len, seq_len)
    attention_mask = padding_mask.unsqueeze(1).expand(-1, seq_len, -1)
    print(f"Padding mask shape: {padding_mask.shape}")
    print(f"Attention mask shape: {attention_mask.shape}")
    
    # ä½¿ç”¨maskçš„å‰å‘ä¼ æ’­
    mha_cache.clear_cache()
    output_with_mask = mha_cache(x, mask=attention_mask, use_cache=False)
    output_without_mask = mha_cache(x, mask=None, use_cache=False)
    
    print(f"ä½¿ç”¨maskæ—¶è¾“å‡ºèŒƒå›´: [{output_with_mask.min():.4f}, {output_with_mask.max():.4f}]")
    print(f"ä¸ä½¿ç”¨maskæ—¶è¾“å‡ºèŒƒå›´: [{output_without_mask.min():.4f}, {output_without_mask.max():.4f}]")
    print(f"è¾“å‡ºå·®å¼‚: {(output_with_mask - output_without_mask).abs().max().item():.6f}")
    print("âœ… MaskåŠŸèƒ½æµ‹è¯•é€šè¿‡")
    
    # æµ‹è¯•6ï¼šæ¢¯åº¦æ£€æŸ¥
    print(f"\n=== æµ‹è¯•6: æ¢¯åº¦æ£€æŸ¥ ===")
    x_grad = torch.randn(batch_size, seq_len, d_model, device=device, requires_grad=True)
    output_grad = mha_cache(x_grad, use_cache=False)
    loss = output_grad.sum()
    loss.backward()
    print(f"è¾“å…¥æ¢¯åº¦èŒƒæ•°: {x_grad.grad.norm().item():.6f}")
    print(f"æƒé‡æ¢¯åº¦å­˜åœ¨: {mha_cache.W_q.weight.grad is not None}")
    print("âœ… æ¢¯åº¦æ£€æŸ¥é€šè¿‡")
    
    # ã€é¢è¯•è€ƒç‚¹20ã€‘ï¼šPyTorch vs NumPyçš„ä¼˜åŠ¿å¯¹æ¯”
    print(f"\n=== PyTorch vs NumPy ä¼˜åŠ¿å¯¹æ¯” ===")
    print("1. è®¾å¤‡çµæ´»æ€§: è‡ªåŠ¨GPUåŠ é€Ÿï¼Œç»Ÿä¸€çš„CPU/GPUæ¥å£")
    print("2. è‡ªåŠ¨å¾®åˆ†: å†…ç½®autogradç³»ç»Ÿï¼Œæ”¯æŒåå‘ä¼ æ’­")
    print("3. å†…å­˜æ•ˆç‡: ä¼˜åŒ–çš„å¼ é‡æ“ä½œå’Œå†…å­˜å¸ƒå±€")
    print("4. ç”Ÿæ€é›†æˆ: ä¸PyTorchç”Ÿæ€ç³»ç»Ÿå®Œç¾é›†æˆ")
    print("5. æ‰¹å¤„ç†ä¼˜åŒ–: é«˜åº¦ä¼˜åŒ–çš„æ‰¹å¤„ç†çŸ©é˜µè¿ç®—")
    
    # ã€é¢è¯•è€ƒç‚¹21ã€‘ï¼šå®é™…éƒ¨ç½²è€ƒè™‘
    print(f"\n=== å®é™…éƒ¨ç½²è€ƒè™‘ ===")
    print("1. æ¨¡å‹é‡åŒ–: å¯ä½¿ç”¨torch.quantizationå‡å°‘å†…å­˜å ç”¨")
    print("2. TorchScript: å¯ç¼–è¯‘ä¸ºç”Ÿäº§ç¯å¢ƒå‹å¥½çš„æ ¼å¼")
    print("3. ONNXå¯¼å‡º: è·¨å¹³å°éƒ¨ç½²æ”¯æŒ")
    print("4. æ··åˆç²¾åº¦: ä½¿ç”¨torch.cuda.ampåŠ é€Ÿè®­ç»ƒ")
    
    print(f"\nâœ… PyTorch Multi-Head Attention with KV Cache æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
    print("ğŸ’¡ é¢è¯•é‡ç‚¹: PyTorchç‰ˆæœ¬åœ¨å·¥ç¨‹å®è·µä¸­æ›´å¸¸ç”¨ï¼Œæ”¯æŒGPUåŠ é€Ÿå’Œè‡ªåŠ¨å¾®åˆ†")


if __name__ == "__main__":
    test_mha_with_kv_cache_torch()