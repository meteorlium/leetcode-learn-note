# PyTorch view() vs reshape() è¯¦è§£

## æ¦‚è¿°
`view()` å’Œ `reshape()` æ˜¯ PyTorch ä¸­ä¸¤ä¸ªå¸¸ç”¨çš„å¼ é‡å½¢çŠ¶å˜æ¢æ–¹æ³•ï¼Œè™½ç„¶åŠŸèƒ½ç›¸ä¼¼ä½†æœ‰é‡è¦åŒºåˆ«ã€‚è¿™æ˜¯é¢è¯•ä¸­çš„é«˜é¢‘è€ƒç‚¹ã€‚

## è®¾è®¡åŠ¨æœºå’Œå†å²èƒŒæ™¯

### view() çš„è®¾è®¡åŠ¨æœº
**åŸå§‹éœ€æ±‚ï¼š** æ·±åº¦å­¦ä¹ ä¸­éœ€è¦é¢‘ç¹æ”¹å˜å¼ é‡å½¢çŠ¶ï¼ˆå¦‚å·ç§¯å±‚è¾“å‡ºå±•å¹³ï¼‰ï¼Œä½†ä¸èƒ½æœ‰é¢å¤–çš„å†…å­˜å¼€é”€å’Œè®¡ç®—æˆæœ¬ã€‚

**è®¾è®¡ç†å¿µï¼š**
```python
# æ ¸å¿ƒæ€æƒ³ï¼šåªæ”¹å˜å¼ é‡çš„"è§‚å¯Ÿæ–¹å¼"ï¼Œä¸ç§»åŠ¨æ•°æ®
# åŸç†ï¼šå¼ é‡åœ¨å†…å­˜ä¸­æ˜¯è¿ç»­å­˜å‚¨çš„ä¸€ç»´æ•°ç»„
data = [1, 2, 3, 4, 5, 6]  # ç‰©ç†å­˜å‚¨

# viewåªæ˜¯æ”¹å˜äº†è§£é‡Šè¿™äº›æ•°æ®çš„æ–¹å¼
tensor_2x3 = view_as([[1, 2, 3],    # 2x3çš„è§†å›¾
                      [4, 5, 6]])
                      
tensor_3x2 = view_as([[1, 2],       # 3x2çš„è§†å›¾
                      [3, 4], 
                      [5, 6]])

# ç‰©ç†æ•°æ®æœªæ”¹å˜ï¼Œåªæ˜¯å…ƒæ•°æ®ï¼ˆshape, strideï¼‰æ”¹å˜äº†
```

**åº•å±‚å®ç°é€»è¾‘ï¼š**
```python
class TensorView:
    def __init__(self, data_ptr, shape, stride):
        self.data_ptr = data_ptr    # æŒ‡å‘åŒä¸€å—å†…å­˜
        self.shape = shape          # æ–°çš„å½¢çŠ¶
        self.stride = stride        # æ–°çš„æ­¥é•¿
        
    def view(self, new_shape):
        # åªæ”¹å˜å…ƒæ•°æ®ï¼Œdata_pträ¸å˜
        new_stride = calculate_stride(new_shape)
        return TensorView(self.data_ptr, new_shape, new_stride)
```

### reshape() çš„è®¾è®¡åŠ¨æœº
**å†å²èƒŒæ™¯ï¼š** 2018å¹´PyTorch 0.4ç‰ˆæœ¬å¼•å…¥ï¼Œä¸»è¦è§£å†³ä¸‰ä¸ªé—®é¢˜ï¼š

1. **NumPyå…¼å®¹æ€§** - è®©PyTorchç”¨æˆ·èƒ½æ— ç¼è¿ç§»NumPyä»£ç 
2. **æ˜“ç”¨æ€§** - é¿å…ç”¨æˆ·è¢«"è¿ç»­æ€§"æ¦‚å¿µå›°æ‰°  
3. **é²æ£’æ€§** - æä¾›æ›´å®‰å…¨çš„å½¢çŠ¶å˜æ¢

**è®¾è®¡ç†å¿µï¼š**
```python
# NumPyçš„reshapeå“²å­¦ï¼šç”¨æˆ·ä¸åº”è¯¥å…³å¿ƒå†…å­˜å¸ƒå±€ç»†èŠ‚
import numpy as np
x = np.array([[1, 2, 3], [4, 5, 6]])
y = x.T.reshape(-1)  # æ€»æ˜¯å¯ä»¥å·¥ä½œï¼Œä¸æŠ¥é”™

# PyTorchæ—©æœŸçš„ç—›ç‚¹
x = torch.tensor([[1, 2, 3], [4, 5, 6]])
y = x.T.view(-1)  # RuntimeErrorï¼ç”¨æˆ·å›°æƒ‘
```

**å®ç°ç­–ç•¥ï¼š**
```python
def reshape(tensor, shape):
    """reshapeçš„å®ç°é€»è¾‘ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    if tensor.is_contiguous():
        # å¿«é€Ÿè·¯å¾„ï¼šç›´æ¥è°ƒç”¨view
        return tensor.view(shape)
    else:
        # å®‰å…¨è·¯å¾„ï¼šå…ˆè°ƒç”¨contiguous()å†view
        return tensor.contiguous().view(shape)
        
# è¿™å°±æ˜¯ä¸ºä»€ä¹ˆreshape"å°½é‡å…±äº«å†…å­˜ï¼Œå¿…è¦æ—¶å¤åˆ¶"
```

## æ·±å±‚è®¡ç®—é€»è¾‘

### å¼ é‡çš„å†…å­˜å¸ƒå±€åŸç†
```python
# ç†è§£strideï¼ˆæ­¥é•¿ï¼‰æ˜¯å…³é”®
import torch

x = torch.arange(24).reshape(2, 3, 4)
print(f"Shape: {x.shape}")      # torch.Size([2, 3, 4])
print(f"Stride: {x.stride()}")  # (12, 4, 1)

# strideå«ä¹‰ï¼š
# - æ²¿ç¬¬0ç»´ç§»åŠ¨1æ­¥ï¼Œå†…å­˜åœ°å€å¢åŠ 12
# - æ²¿ç¬¬1ç»´ç§»åŠ¨1æ­¥ï¼Œå†…å­˜åœ°å€å¢åŠ 4  
# - æ²¿ç¬¬2ç»´ç§»åŠ¨1æ­¥ï¼Œå†…å­˜åœ°å€å¢åŠ 1

# è®¿é—®å…ƒç´ x[i,j,k]çš„å†…å­˜åç§» = i*12 + j*4 + k*1
```

### view() çš„æ•°å­¦çº¦æŸ
```python
# view()èƒ½æˆåŠŸçš„æ•°å­¦æ¡ä»¶
def can_view(original_shape, original_stride, new_shape):
    """åˆ¤æ–­æ˜¯å¦å¯ä»¥viewçš„ç®—æ³•"""
    
    # 1. å…ƒç´ æ€»æ•°å¿…é¡»ç›¸ç­‰
    if np.prod(original_shape) != np.prod(new_shape):
        return False
    
    # 2. å¿…é¡»å­˜åœ¨åˆæ³•çš„strideæ˜ å°„
    # æ–°strideå¿…é¡»èƒ½é€šè¿‡åŸstrideè®¡ç®—å¾—å‡º
    
    # 3. å†…å­˜è®¿é—®æ¨¡å¼å¿…é¡»ä¿æŒçº¿æ€§
    # è¿™æ˜¯æœ€å¤æ‚çš„çº¦æŸï¼Œæ¶‰åŠstrideçš„å…¼å®¹æ€§æ£€æŸ¥
    
    return check_stride_compatibility(original_stride, new_shape)

# è¿™å°±æ˜¯ä¸ºä»€ä¹ˆtransposeåä¸èƒ½ç›´æ¥view
x = torch.randn(2, 3)
print(x.stride())        # (3, 1) - æ­£å¸¸çš„è¡Œä¼˜å…ˆå­˜å‚¨

x_t = x.T
print(x_t.stride())      # (1, 3) - å˜æˆåˆ—ä¼˜å…ˆï¼Œç ´åäº†è¿ç»­æ€§
```

### è¿ç»­æ€§çš„æ·±å±‚å«ä¹‰
```python
# è¿ç»­æ€§ä¸åªæ˜¯"æ˜¯å¦æŒ‰è¡Œå­˜å‚¨"ï¼Œè€Œæ˜¯strideçš„è§„å¾‹æ€§
def is_contiguous_detailed(tensor):
    """è¯¦ç»†çš„è¿ç»­æ€§æ£€æŸ¥"""
    shape = tensor.shape
    stride = tensor.stride()
    
    # è¿ç»­å­˜å‚¨çš„strideåº”è¯¥æ»¡è¶³ï¼š
    # stride[i] = stride[i+1] * shape[i+1]
    
    expected_stride = [1]
    for i in range(len(shape)-2, -1, -1):
        expected_stride.insert(0, expected_stride[0] * shape[i+1])
    
    return list(stride) == expected_stride

# ä¸¾ä¾‹è¯´æ˜
x = torch.randn(2, 3, 4)
print(f"è¿ç»­: {is_contiguous_detailed(x)}")  # True
# æœŸæœ›stride: [12, 4, 1] = [1*3*4, 1*4, 1]

x_t = x.transpose(0, 1)  
print(f"è½¬ç½®åè¿ç»­: {is_contiguous_detailed(x_t)}")  # False
# å®é™…stride: [4, 12, 1]ï¼Œä¸ç¬¦åˆè¿ç»­æ€§è§„å¾‹
```

## å·¥ç¨‹è®¾è®¡æƒè¡¡

### PyTorchå›¢é˜Ÿçš„è®¾è®¡è€ƒé‡

**1. æ€§èƒ½ vs æ˜“ç”¨æ€§**
```python
# è®¾è®¡å†³ç­–ï¼šæä¾›ä¸¤ä¸ªAPIè€Œä¸æ˜¯ä¸€ä¸ª
# 
# å¦‚æœåªæœ‰reshape():
# - ä¼˜ç‚¹ï¼šç”¨æˆ·å‹å¥½ï¼Œä¸ä¼šå‡ºé”™
# - ç¼ºç‚¹ï¼šéšè—äº†æ€§èƒ½æˆæœ¬ï¼Œå¯èƒ½æ„å¤–å¤åˆ¶æ•°æ®
#
# å¦‚æœåªæœ‰view():  
# - ä¼˜ç‚¹ï¼šæ€§èƒ½é€æ˜ï¼Œç”¨æˆ·çŸ¥é“æˆæœ¬
# - ç¼ºç‚¹ï¼šå­¦ä¹ æ›²çº¿é™¡å³­ï¼Œå®¹æ˜“å‡ºé”™
#
# æœ€ç»ˆæ–¹æ¡ˆï¼šä¸¤è€…å¹¶å­˜
# - view(): ä¸“å®¶ç”¨æˆ·ï¼Œæ€§èƒ½æ•æ„Ÿåœºæ™¯
# - reshape(): æ™®é€šç”¨æˆ·ï¼ŒåŸå‹å¼€å‘
```

**2. å†…å­˜ç®¡ç†å“²å­¦**
```python
# PyTorchçš„å†…å­˜ç®¡ç†ç†å¿µ
class MemoryPhilosophy:
    """
    1. æ˜¾å¼ > éšå¼ï¼šç”¨æˆ·åº”è¯¥çŸ¥é“æ“ä½œçš„æˆæœ¬
    2. é›¶æ‹·è´ä¼˜å…ˆï¼šå°½é‡é¿å…ä¸å¿…è¦çš„å†…å­˜åˆ†é…
    3. å®‰å…¨ç½‘ï¼šæä¾›å®‰å…¨çš„fallbacké€‰é¡¹
    """
    
    def view_philosophy(self):
        """viewçš„å“²å­¦ï¼šæ˜¾å¼çš„é›¶æ‹·è´"""
        # ç”¨æˆ·æ˜ç¡®çŸ¥é“ï¼š
        # - è¿™ä¸ªæ“ä½œæ˜¯O(1)æ—¶é—´å¤æ‚åº¦
        # - ä¿®æ”¹ç»“æœä¼šå½±å“åŸå¼ é‡
        # - å¦‚æœå¤±è´¥ï¼Œè¯´æ˜éœ€è¦å¤„ç†è¿ç»­æ€§
        pass
    
    def reshape_philosophy(self):
        """reshapeçš„å“²å­¦ï¼šæ™ºèƒ½çš„å†…å­˜ç®¡ç†"""
        # æ¡†æ¶è´Ÿè´£ï¼š
        # - è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç­–ç•¥
        # - éšè—å®ç°ç»†èŠ‚
        # - ä¿è¯æ“ä½œæ€»æ˜¯æˆåŠŸ
        pass
```

### æ·±åº¦å­¦ä¹ åœºæ™¯çš„å…·ä½“éœ€æ±‚

**1. å‰å‘ä¼ æ’­ä¸­çš„å½¢çŠ¶å˜æ¢**
```python
# å…¸å‹åœºæ™¯ï¼šCNNåˆ°FCå±‚çš„è¿‡æ¸¡
class CNNToFC(nn.Module):
    def forward(self, x):
        # x: (batch, channels, height, width)
        x = self.conv_layers(x)  # (batch, 512, 7, 7)
        
        # éœ€è¦å±•å¹³ç»™å…¨è¿æ¥å±‚
        # æ–¹æ¡ˆ1ï¼šä½¿ç”¨viewï¼ˆå¸¸è§åšæ³•ï¼‰
        x = x.view(x.size(0), -1)  # (batch, 512*7*7)
        
        # ä¸ºä»€ä¹ˆç”¨viewï¼Ÿ
        # 1. convè¾“å‡ºæ€»æ˜¯è¿ç»­çš„
        # 2. æ€§èƒ½æ•æ„Ÿï¼Œé¿å…ä¸å¿…è¦çš„å¼€é”€
        # 3. è¿™æ˜¯æ¡†æ¶å†…éƒ¨ï¼Œå¼€å‘è€…äº†è§£è¿ç»­æ€§
        
        return self.fc(x)
```

**2. æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„ç»´åº¦å˜æ¢**
```python
# å¤šå¤´æ³¨æ„åŠ›çš„å¤æ‚å½¢çŠ¶å˜æ¢
def multi_head_attention_shapes():
    # è¾“å…¥: (batch, seq_len, d_model)
    batch, seq_len, d_model = 32, 128, 512
    num_heads = 8
    
    x = torch.randn(batch, seq_len, d_model)
    
    # çº¿æ€§å˜æ¢åéœ€è¦åˆ†å¤´
    # (batch, seq_len, d_model) -> (batch, seq_len, num_heads, d_k)
    
    # ä¸ºä»€ä¹ˆç”¨viewï¼Ÿ
    # 1. çº¿æ€§å±‚è¾“å‡ºæ˜¯è¿ç»­çš„
    # 2. è¿™ä¸ªæ“ä½œåœ¨è®­ç»ƒä¸­è¢«è°ƒç”¨æ•°ç™¾ä¸‡æ¬¡
    # 3. ä»»ä½•é¢å¤–å¼€é”€éƒ½ä¼šæ˜¾è‘—å½±å“è®­ç»ƒæ—¶é—´
    
    q = x.view(batch, seq_len, num_heads, d_model // num_heads)
    
    # ç„¶åtransposeç”¨äºçŸ©é˜µä¹˜æ³•
    # (batch, seq_len, num_heads, d_k) -> (batch, num_heads, seq_len, d_k)
    q = q.transpose(1, 2)  # ç°åœ¨å˜æˆéè¿ç»­çš„äº†
    
    # å¦‚æœåç»­éœ€è¦reshapeï¼Œå¿…é¡»ç”¨reshapeè€Œä¸æ˜¯view
    # q_flat = q.reshape(batch * num_heads, seq_len, d_k)  # âœ…
    # q_flat = q.view(batch * num_heads, seq_len, d_k)     # âŒ
```

### æ¡†æ¶æ¼”è¿›çš„å†å²é€»è¾‘

**1. PyTorchæ—©æœŸï¼ˆ2016-2017ï¼‰ï¼šåªæœ‰view()**
```python
# æ—©æœŸç”¨æˆ·çš„ç—›è‹¦ç»å†
def early_pytorch_pain():
    x = torch.randn(2, 3, 4)
    
    # è¿™äº›æ“ä½œç»å¸¸è®©æ–°æ‰‹å›°æƒ‘
    try:
        y = x.transpose(0, 1).view(-1)  # æŠ¥é”™ï¼
    except:
        # ç”¨æˆ·éœ€è¦å­¦ä¼šè¿™æ ·å†™
        y = x.transpose(0, 1).contiguous().view(-1)
        
    # é—®é¢˜ï¼š
    # 1. å­¦ä¹ æ›²çº¿é™¡å³­
    # 2. é”™è¯¯ä¿¡æ¯ä¸å‹å¥½  
    # 3. ä¸NumPyå·®å¼‚å¤ªå¤§
```

**2. PyTorch 0.4ï¼ˆ2018ï¼‰ï¼šå¼•å…¥reshape()**
```python
# è§£å†³æ–¹æ¡ˆï¼šæ·»åŠ reshape()ä½œä¸ºç”¨æˆ·å‹å¥½çš„API
def pytorch_0_4_solution():
    # ç°åœ¨ç”¨æˆ·å¯ä»¥è¿™æ ·å†™
    x = torch.randn(2, 3, 4)
    y = x.transpose(0, 1).reshape(-1)  # æ€»æ˜¯å¯ä»¥å·¥ä½œ
    
    # åŒæ—¶ä¿ç•™view()ç»™æ€§èƒ½æ•æ„Ÿçš„åœºæ™¯
    z = x.view(-1)  # é«˜æ€§èƒ½ï¼Œä½†è¦æ±‚è¿ç»­æ€§
```

**3. ç°ä»£PyTorchï¼šæœ€ä½³å®è·µç¡®ç«‹**
```python
# ç°åœ¨çš„æœ€ä½³å®è·µ
class ModernBestPractices:
    def framework_internal(self, x):
        """æ¡†æ¶å†…éƒ¨ï¼šä½¿ç”¨view"""
        # ç¡®å®šè¿ç»­æ€§ï¼Œè¿½æ±‚æœ€é«˜æ€§èƒ½
        return x.view(new_shape)
    
    def user_code_prototype(self, x):
        """ç”¨æˆ·ä»£ç /åŸå‹ï¼šä½¿ç”¨reshape"""
        # ä¸ç¡®å®šè¿ç»­æ€§ï¼Œè¿½æ±‚ç¨³å®šæ€§
        return x.reshape(new_shape)
    
    def performance_critical(self, x):
        """æ€§èƒ½å…³é”®è·¯å¾„ï¼šæ˜¾å¼å¤„ç†"""
        if not x.is_contiguous():
            x = x.contiguous()
        return x.view(new_shape)
```

## æ€»ç»“ï¼šè®¾è®¡åŠ¨æœºçš„æ·±å±‚ç†è§£

**view()çš„æœ¬è´¨**ï¼š
- è¿™æ˜¯ä¸€ä¸ª"é›¶æˆæœ¬æŠ½è±¡"ï¼ˆzero-cost abstractionï¼‰
- ä½“ç°äº†ç³»ç»Ÿç¼–ç¨‹çš„ç†å¿µï¼šç»™ä¸“å®¶å®Œå…¨çš„æ§åˆ¶æƒ
- ç±»ä¼¼C++çš„reinterpret_castï¼šå¿«é€Ÿä½†éœ€è¦ä¸“ä¸šçŸ¥è¯†

**reshape()çš„æœ¬è´¨**ï¼š
- è¿™æ˜¯ä¸€ä¸ª"æ™ºèƒ½åŒ…è£…å™¨"ï¼ˆsmart wrapperï¼‰
- ä½“ç°äº†ç”¨æˆ·å‹å¥½çš„ç†å¿µï¼šè®©æ¡†æ¶å¤„ç†å¤æ‚æ€§
- ç±»ä¼¼é«˜çº§è¯­è¨€çš„è‡ªåŠ¨å†…å­˜ç®¡ç†ï¼šå®‰å…¨ä½†å¯èƒ½æœ‰éšè—æˆæœ¬

**ä¸ºä»€ä¹ˆéœ€è¦ä¸¤è€…**ï¼š
- ä¸åŒç”¨æˆ·ç¾¤ä½“æœ‰ä¸åŒéœ€æ±‚
- ä¸åŒä½¿ç”¨åœºæ™¯æœ‰ä¸åŒä¼˜å…ˆçº§
- æ¡†æ¶çš„æˆç†Ÿæ ‡å¿—æ˜¯èƒ½å¤Ÿå¹³è¡¡ä¸“ä¸šæ€§å’Œæ˜“ç”¨æ€§

è¿™ç§è®¾è®¡åæ˜ äº†PyTorchä½œä¸º"ç ”ç©¶ä¼˜å…ˆ"æ¡†æ¶çš„å“²å­¦ï¼šç»™ç ”ç©¶è€…æœ€å¤§çš„çµæ´»æ€§å’Œæ§åˆ¶æƒï¼ŒåŒæ—¶ä¸å¿½è§†å·¥ç¨‹å®ç”¨æ€§ã€‚

## æ ¸å¿ƒåŒºåˆ«å¯¹æ¯”

| ç‰¹æ€§ | view() | reshape() |
|------|--------|-----------|
| **è¿ç»­æ€§è¦æ±‚** | å¿…é¡»è¿ç»­ | è‡ªåŠ¨å¤„ç† |
| **å†…å­˜å…±äº«** | æ€»æ˜¯å…±äº« | å°½é‡å…±äº« |
| **æ€§èƒ½** | æœ€å¿« | ç¨æ…¢ |
| **å®‰å…¨æ€§** | ä¸¥æ ¼æ£€æŸ¥ | æ›´å®½æ¾ |
| **å¼•å…¥ç‰ˆæœ¬** | æ—©æœŸç‰ˆæœ¬ | PyTorch 0.4+ |

## 1. è¿ç»­æ€§è¦æ±‚

### view() - ä¸¥æ ¼è¦æ±‚è¿ç»­æ€§
```python
import torch

# è¿ç»­å¼ é‡ - viewæ­£å¸¸å·¥ä½œ
x = torch.randn(2, 3, 4)
print(f"è¿ç»­æ€§: {x.is_contiguous()}")  # True
y = x.view(6, 4)  # âœ… æˆåŠŸ

# éè¿ç»­å¼ é‡ - viewå¤±è´¥
x_t = x.transpose(0, 1)  # è½¬ç½®åéè¿ç»­
print(f"è½¬ç½®åè¿ç»­æ€§: {x_t.is_contiguous()}")  # False
try:
    y = x_t.view(6, 4)  # âŒ æŠ¥é”™
except RuntimeError as e:
    print(f"viewé”™è¯¯: {e}")
    # RuntimeError: view size is not compatible with input tensor's size and stride
```

### reshape() - è‡ªåŠ¨å¤„ç†éè¿ç»­æ€§
```python
# reshapeè‡ªåŠ¨å¤„ç†éè¿ç»­æƒ…å†µ
x = torch.randn(2, 3, 4)
x_t = x.transpose(0, 1)  # éè¿ç»­

y = x_t.reshape(6, 4)  # âœ… æˆåŠŸï¼Œè‡ªåŠ¨è°ƒç”¨contiguous()
print(f"reshapeæˆåŠŸ: {y.shape}")
```

## 2. å†…å­˜å…±äº«è¡Œä¸º

### view() - æ€»æ˜¯å…±äº«å†…å­˜
```python
x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)
x_view = x.view(3, 2)

# ä¿®æ”¹åŸå¼ é‡
x[0, 0] = 999
print(x_view)  # ç¬¬ä¸€ä¸ªå…ƒç´ ä¹Ÿå˜æˆ999ï¼Œè¯´æ˜å…±äº«å†…å­˜

# æ£€æŸ¥å†…å­˜åœ°å€
print(f"å…±äº«å†…å­˜: {x.data_ptr() == x_view.data_ptr()}")  # True
```

### reshape() - å°½é‡å…±äº«ï¼Œå¿…è¦æ—¶å¤åˆ¶
```python
# è¿ç»­å¼ é‡ - reshapeå…±äº«å†…å­˜
x = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)
x_reshape = x.reshape(3, 2)
print(f"è¿ç»­æ—¶å…±äº«å†…å­˜: {x.data_ptr() == x_reshape.data_ptr()}")  # True

# éè¿ç»­å¼ é‡ - reshapeå¯èƒ½å¤åˆ¶
x_t = x.transpose(0, 1)
x_reshape = x_t.reshape(3, 2)
print(f"éè¿ç»­æ—¶å…±äº«å†…å­˜: {x_t.data_ptr() == x_reshape.data_ptr()}")  # False
```

## 3. æ€§èƒ½å¯¹æ¯”

### åŸºå‡†æµ‹è¯•
```python
import time
import torch

x = torch.randn(1000, 1000)

# viewæ€§èƒ½æµ‹è¯•
start = time.time()
for _ in range(10000):
    _ = x.view(1000000)
view_time = time.time() - start

# reshapeæ€§èƒ½æµ‹è¯•  
start = time.time()
for _ in range(10000):
    _ = x.reshape(1000000)
reshape_time = time.time() - start

print(f"viewå¹³å‡ç”¨æ—¶: {view_time:.6f}s")    # æ›´å¿«
print(f"reshapeå¹³å‡ç”¨æ—¶: {reshape_time:.6f}s")  # ç¨æ…¢
```

**ç»“æœåˆ†æ**ï¼š
- `view()` æ›´å¿«ï¼šç›´æ¥æ”¹å˜å¼ é‡çš„viewï¼Œæ— é¢å¤–æ£€æŸ¥
- `reshape()` ç¨æ…¢ï¼šéœ€è¦æ£€æŸ¥è¿ç»­æ€§ï¼Œå¯èƒ½è°ƒç”¨`contiguous()`ï¼ˆçº¦1-3%çš„æ€§èƒ½å·®å¼‚ï¼‰

## 4. å®é™…ä½¿ç”¨ç¤ºä¾‹

### åœ¨å¤šå¤´æ³¨æ„åŠ›ä¸­çš„åº”ç”¨
```python
class MultiHeadAttention(nn.Module):
    def forward(self, x):
        batch_size, seq_len, d_model = x.shape
        
        # æ–¹æ³•1: ä½¿ç”¨viewï¼ˆæ¨èï¼‰
        # è¦æ±‚è¾“å…¥å¿…é¡»è¿ç»­ï¼Œæ€§èƒ½æœ€ä½³
        Q = self.W_q(x).view(batch_size, seq_len, self.num_heads, self.d_k)
        
        # æ–¹æ³•2: ä½¿ç”¨reshapeï¼ˆæ›´å®‰å…¨ï¼‰
        # è‡ªåŠ¨å¤„ç†è¿ç»­æ€§ï¼Œä½†ç¨æ…¢
        Q = self.W_q(x).reshape(batch_size, seq_len, self.num_heads, self.d_k)
        
        return Q
```

### ä½•æ—¶ä½¿ç”¨å“ªä¸ªï¼Ÿ
```python
# æ¨èä½¿ç”¨viewçš„æƒ…å†µ
def use_view_when():
    # 1. ç¡®å®šè¾“å…¥æ˜¯è¿ç»­çš„
    x = torch.randn(2, 3, 4)  # æ–°åˆ›å»ºçš„å¼ é‡æ€»æ˜¯è¿ç»­çš„
    y = x.view(-1, 4)  # âœ… å®‰å…¨
    
    # 2. æ€§èƒ½æ•æ„Ÿçš„ä»£ç 
    for _ in range(1000000):
        y = x.view(6, 4)  # æ›´å¿«
    
    # 3. éœ€è¦å†…å­˜å…±äº«çš„åœºæ™¯
    x_view = x.view(-1)
    x_view[0] = 999  # ä¿®æ”¹viewä¼šå½±å“åŸå¼ é‡

# æ¨èä½¿ç”¨reshapeçš„æƒ…å†µ  
def use_reshape_when():
    # 1. ä¸ç¡®å®šè¾“å…¥è¿ç»­æ€§
    x = some_complex_operation()  # å¯èƒ½éè¿ç»­
    y = x.reshape(-1, 4)  # âœ… å®‰å…¨ï¼Œè‡ªåŠ¨å¤„ç†
    
    # 2. ç¼–å†™é€šç”¨ä»£ç 
    def generic_function(tensor):
        return tensor.reshape(-1)  # é€‚ç”¨äºä»»ä½•è¾“å…¥
    
    # 3. åŸå‹å¼€å‘é˜¶æ®µ
    y = x.reshape(new_shape)  # æ›´å°‘å‡ºé”™
```

## 5. å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

### é”™è¯¯1ï¼šéè¿ç»­å¼ é‡ä½¿ç”¨view
```python
# âŒ é”™è¯¯ä»£ç 
x = torch.randn(2, 3, 4)
x_t = x.transpose(0, 1)
y = x_t.view(6, 4)  # RuntimeError

# âœ… è§£å†³æ–¹æ¡ˆ1ï¼šä½¿ç”¨reshape
y = x_t.reshape(6, 4)

# âœ… è§£å†³æ–¹æ¡ˆ2ï¼šå…ˆè°ƒç”¨contiguous()
y = x_t.contiguous().view(6, 4)
```

### é”™è¯¯2ï¼šå…ƒç´ æ•°é‡ä¸åŒ¹é…
```python
# âŒ é”™è¯¯ä»£ç 
x = torch.randn(2, 3, 4)  # 24ä¸ªå…ƒç´ 
y = x.view(5, 5)  # 25ä¸ªå…ƒç´ ï¼ŒæŠ¥é”™

# âœ… æ­£ç¡®ä»£ç 
y = x.view(6, 4)   # 24ä¸ªå…ƒç´ 
y = x.view(-1, 4)  # è‡ªåŠ¨è®¡ç®—ï¼š24/4=6
y = x.view(2, -1)  # è‡ªåŠ¨è®¡ç®—ï¼š24/2=12
```

### é”™è¯¯3ï¼šå¤šä¸ª-1
```python
# âŒ é”™è¯¯ä»£ç 
x = torch.randn(2, 3, 4)
y = x.view(-1, -1)  # æœ€å¤šåªèƒ½æœ‰ä¸€ä¸ª-1

# âœ… æ­£ç¡®ä»£ç 
y = x.view(-1, 4)   # ä¸€ä¸ª-1
y = x.view(6, -1)   # ä¸€ä¸ª-1
```

## 6. ç‰ˆæœ¬å˜åŒ–å’Œå…¼å®¹æ€§

### PyTorchç‰ˆæœ¬å†å²
```python
# PyTorch < 0.4 (2018å¹´ä¹‹å‰)
# åªæœ‰view()æ–¹æ³•ï¼Œæ²¡æœ‰reshape()

# PyTorch 0.4+ (2018å¹´4æœˆ)
# å¼•å…¥reshape()æ–¹æ³•ï¼Œä¸NumPyå…¼å®¹

# PyTorch 1.0+ (2018å¹´12æœˆ)
# reshape()åŠŸèƒ½ç¨³å®šï¼Œæ¨èä½¿ç”¨

# PyTorch 1.7+ (2020å¹´10æœˆ)  
# æ€§èƒ½ä¼˜åŒ–ï¼Œreshape()å¼€é”€è¿›ä¸€æ­¥é™ä½
```

### å‘åå…¼å®¹æ€§
```python
# æ£€æŸ¥PyTorchç‰ˆæœ¬
import torch
print(torch.__version__)

# å…¼å®¹æ€§ä»£ç 
def safe_reshape(tensor, shape):
    """å…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬çš„reshape"""
    if hasattr(tensor, 'reshape'):
        return tensor.reshape(shape)
    else:
        # è€ç‰ˆæœ¬fallback
        return tensor.contiguous().view(shape)
```

## 7. ä¸NumPyçš„å¯¹æ¯”

### NumPy vs PyTorch
```python
import numpy as np
import torch

# NumPyåªæœ‰reshape
np_array = np.random.randn(2, 3, 4)
np_reshaped = np_array.reshape(6, 4)  # æ€»æ˜¯å¯ç”¨

# PyTorchä¸¤ç§é€‰æ‹©
torch_tensor = torch.randn(2, 3, 4)
torch_viewed = torch_tensor.view(6, 4)      # PyTorchç‰¹æœ‰
torch_reshaped = torch_tensor.reshape(6, 4)  # ä¸NumPyå…¼å®¹

# ä»NumPyè¿ç§»åˆ°PyTorch
# np.reshape() -> torch.reshape() (æ¨è)
# æˆ–è€…ä½¿ç”¨view()è·å¾—æ›´å¥½æ€§èƒ½
```

## 8. é¢è¯•å¸¸è€ƒé—®é¢˜

### Q1: viewå’Œreshapeçš„ä¸»è¦åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
**æ ‡å‡†ç­”æ¡ˆ**ï¼š
1. **è¿ç»­æ€§è¦æ±‚**ï¼šviewè¦æ±‚å¼ é‡è¿ç»­ï¼Œreshapeè‡ªåŠ¨å¤„ç†
2. **æ€§èƒ½**ï¼šviewæ›´å¿«ï¼Œreshapeç¨æ…¢ä½†æ›´å®‰å…¨
3. **å†…å­˜**ï¼šviewæ€»æ˜¯å…±äº«å†…å­˜ï¼Œreshapeå°½é‡å…±äº«

### Q2: ä»€ä¹ˆæ—¶å€™ä¼šå‡ºç°éè¿ç»­å¼ é‡ï¼Ÿ
**æ ‡å‡†ç­”æ¡ˆ**ï¼š
```python
# å¸¸è§çš„éè¿ç»­æ“ä½œ
x = torch.randn(2, 3, 4)

# 1. transpose/permute
x_t = x.transpose(0, 1)  # éè¿ç»­

# 2. åˆ‡ç‰‡æ“ä½œ
x_slice = x[:, ::2, :]   # å¯èƒ½éè¿ç»­

# 3. å±•å¼€æ“ä½œ
x_narrow = x.narrow(1, 0, 2)  # éè¿ç»­
```

### Q3: å¦‚ä½•æ£€æŸ¥å’Œä¿®å¤è¿ç»­æ€§ï¼Ÿ
**æ ‡å‡†ç­”æ¡ˆ**ï¼š
```python
# æ£€æŸ¥è¿ç»­æ€§
print(tensor.is_contiguous())

# ä¿®å¤è¿ç»­æ€§
tensor_continuous = tensor.contiguous()

# æˆ–è€…ç›´æ¥ä½¿ç”¨reshape
tensor_reshaped = tensor.reshape(new_shape)
```

## 9. æœ€ä½³å®è·µ

### æ¨èåšæ³•
```python
class BestPractices:
    def __init__(self):
        pass
    
    def high_performance_code(self, x):
        """æ€§èƒ½æ•æ„Ÿçš„ä»£ç ä½¿ç”¨view"""
        # ç¡®å®šè¾“å…¥è¿ç»­æ—¶ä½¿ç”¨view
        return x.view(batch_size, -1)
    
    def generic_code(self, x):
        """é€šç”¨ä»£ç ä½¿ç”¨reshape"""
        # ä¸ç¡®å®šè¾“å…¥æ—¶ä½¿ç”¨reshape
        return x.reshape(batch_size, -1)
    
    def safe_view(self, x, shape):
        """å®‰å…¨çš„viewæ“ä½œ"""
        if not x.is_contiguous():
            x = x.contiguous()
        return x.view(shape)
```

### è°ƒè¯•æŠ€å·§
```python
def debug_tensor_shape(tensor, name="tensor"):
    """è°ƒè¯•å¼ é‡å½¢çŠ¶ä¿¡æ¯"""
    print(f"{name}:")
    print(f"  shape: {tensor.shape}")
    print(f"  stride: {tensor.stride()}")
    print(f"  is_contiguous: {tensor.is_contiguous()}")
    print(f"  element_size: {tensor.element_size()}")
    print(f"  storage_size: {tensor.storage().size()}")
```

## æ€»ç»“

**é€‰æ‹©æŒ‡å—**ï¼š
- ğŸš€ **æ€§èƒ½ä¼˜å…ˆ**ï¼šä½¿ç”¨ `view()`ï¼Œä½†ç¡®ä¿è¾“å…¥è¿ç»­
- ğŸ›¡ï¸ **å®‰å…¨ä¼˜å…ˆ**ï¼šä½¿ç”¨ `reshape()`ï¼Œè‡ªåŠ¨å¤„ç†å„ç§æƒ…å†µ
- ğŸ”„ **NumPyè¿ç§»**ï¼šä½¿ç”¨ `reshape()` ä¿æŒä¸€è‡´æ€§
- ğŸ“š **å­¦ä¹ é˜¶æ®µ**ï¼šä½¿ç”¨ `reshape()` é¿å…è¸©å‘

**é¢è¯•é‡ç‚¹**ï¼š
1. ç†è§£è¿ç»­æ€§æ¦‚å¿µå’Œæ£€æŸ¥æ–¹æ³•
2. æŒæ¡ä¸¤è€…çš„æ€§èƒ½å’Œå†…å­˜è¡Œä¸ºå·®å¼‚
3. èƒ½å¤Ÿé€‰æ‹©åˆé€‚çš„æ–¹æ³•è§£å†³å®é™…é—®é¢˜
4. äº†è§£å¸¸è§é”™è¯¯å’Œè°ƒè¯•æŠ€å·§